{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FGV EMAP\n",
    "\n",
    "## Modelagem e mineração de dados\n",
    "\n",
    "\n",
    "### Trabalho Kaggle Quora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alunos: Antonio Sombra e Joao Marcos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#strings\n",
    "import string\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "#Extras\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "#basic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import string\n",
    "import math\n",
    "\n",
    "\n",
    "# Visualisation\n",
    "import pylab\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
<<<<<<< HEAD
       "      <th>399590</th>\n",
       "      <td>Which is the best CAT coaching centre in bangl...</td>\n",
       "      <td>Which is the best CAT coaching centre in Pune?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130118</th>\n",
       "      <td>Man that wants a pussy?</td>\n",
       "      <td>How is an image captured by the camera?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367805</th>\n",
       "      <td>How do I delete sent pictures on chat for Snap...</td>\n",
       "      <td>How do I retrieve deleted Snapchat messages?</td>\n",
       "      <td>0</td>\n",
=======
       "      <th>77955</th>\n",
       "      <td>What are the main inspiratory muscles?</td>\n",
       "      <td>What are the main muscles of the body?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268481</th>\n",
       "      <td>What are the easy ways to earn money online?</td>\n",
       "      <td>How can we earn money online without investment?</td>\n",
       "      <td>1</td>\n",
>>>>>>> parent of e831bcf... mais uma
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315316</th>\n",
       "      <td>What songs are used for the drum patterns of t...</td>\n",
       "      <td>Is drum and bass a popular music genre in the US?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question1  \\\n",
<<<<<<< HEAD
       "399590  Which is the best CAT coaching centre in bangl...   \n",
       "130118                            Man that wants a pussy?   \n",
       "367805  How do I delete sent pictures on chat for Snap...   \n",
       "\n",
       "                                             question2  is_duplicate  \n",
       "399590  Which is the best CAT coaching centre in Pune?             0  \n",
       "130118         How is an image captured by the camera?             0  \n",
       "367805    How do I retrieve deleted Snapchat messages?             0  "
=======
       "77955              What are the main inspiratory muscles?   \n",
       "268481       What are the easy ways to earn money online?   \n",
       "315316  What songs are used for the drum patterns of t...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "77955              What are the main muscles of the body?             0  \n",
       "268481   How can we earn money online without investment?             1  \n",
       "315316  Is drum and bass a popular music genre in the US?             0  "
>>>>>>> parent of e831bcf... mais uma
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = \"/Dados/Kaggle/\"\n",
    "word2vec= \"/Dados/Word2vec\"\n",
    "save=  os.getcwd()\n",
    "data_train = pd.read_csv(os.path.join(datapath, 'train.csv'))\n",
    "data_train=data_train.drop(['id','qid1','qid2'],axis=1)\n",
    "data_train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404290 entries, 0 to 404289\n",
      "Data columns (total 3 columns):\n",
      "question1       404290 non-null object\n",
      "question2       404288 non-null object\n",
      "is_duplicate    404290 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 9.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404290.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.369198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.482588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_duplicate\n",
       "count  404290.000000\n",
       "mean        0.369198\n",
       "std         0.482588\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         1.000000\n",
       "max         1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.info()\n",
    "data_train.describe()\n",
    "\n",
    "#0.369198 percente is duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['question1', 'question2', 'is_duplicate'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_train.columns)\n",
    "type(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choosing the list of stopwords\n",
    "mystopwords = nltk.corpus.stopwords.words('english')\n",
    "list_of_words= ['where','what','when','why','between','who','how','which']\n",
    "for item in list_of_words:\n",
    "        mystopwords.remove(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing questions with NLTK package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the step by step guid to invest in share market in india?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(phrase,list_stopwords):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes all stopwords from a list\n",
    "    :param phrase: String. A phrase.\n",
    "    :param list_stopwords: List. A list of stopwords\n",
    "    :return: The same phrase without stopwords\n",
    "    \"\"\"\n",
    "    final_phrase = []\n",
    "    words = phrase.split(\" \")\n",
    "    for word in words:\n",
    "        if word not in list_stopwords:\n",
    "            final_phrase.append((word))\n",
    "    \n",
    "    final_phrase = ' '.join(final_phrase)\n",
    "    \n",
    "    return final_phrase\n",
    "    \n",
    "def remove_punctuation(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes all punctuation from it\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase without punctuation\n",
    "    \"\"\"\n",
    "    #https://www.tutorialspoint.com/python/string_maketrans.htm\n",
    "    #Check if NA\n",
    "    if type(phrase) is float:\n",
    "        if math.isnan(phrase):\n",
    "            return (\"\")\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    phrase = phrase.translate(translator) #removing punctuation\n",
    "        \n",
    "    return phrase\n",
    "\n",
    "def lemm_wordnet(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes lemmatizes it\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase in lemmas\n",
    "    \"\"\"\n",
    "    lemm = WordNetLemmatizer()\n",
    "    \n",
    "    #NA is a float type, so this if is to avoid conflict\n",
    "    if type(phrase) is not float:\n",
    "        phrase = [lemm.lemmatize(i) for i in phrase.split()]\n",
    "        phrase = ' '.join(phrase)\n",
    "    else:\n",
    "        return \"\"\n",
    "    return phrase\n",
    "    \n",
    "def remove_duplicate(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes all duplicate words\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase with just unique words\n",
    "    \"\"\"\n",
    "    aux_phrase = []\n",
    "        \n",
    "    if type(phrase) is not float:\n",
    "        \n",
    "        for i in phrase.split():\n",
    "            \n",
    "            if i not in aux_phrase:\n",
    "                aux_phrase.append(i)\n",
    "    \n",
    "    phrase = ' '.join(aux_phrase)\n",
    "    \n",
    "    return phrase\n",
    "    \n",
    "    \n",
    "def all_lower_case(phrase):    \n",
    "    \"\"\"\n",
    "    Receives a phrase and makes it lower case\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase in lower case\n",
    "    \"\"\"\n",
    "    if type(phrase) is not float:\n",
    "            phrase = phrase.lower()\n",
    "    return phrase\n",
    "    \n",
    "def stem_snowball(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and returns the same phrase stemmed, lowercase phrase without stopwords\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: String. Stemmed, lowercase phrase without stopwords\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    #Stem words according to stemmer\n",
    "    final_phrase = []\n",
    "    words = phrase.split(\" \")\n",
    "    for word in words:\n",
    "        final_phrase.append((stemmer.stem(word)))\n",
    "    \n",
    "    final_phrase = ' '.join(final_phrase)\n",
    "    \n",
    "    return final_phrase\n",
    "\n",
    "stem_snowball(\"What is the step by step guide to invest in share market in india?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuzz_qratio\n",
      "fuzz_WRatio\n",
      "fuzz_partial_ratio\n",
      "fuzz_partial_token_set_ratio\n",
      "fuzz_partial_token_sort_ratio\n",
      "fuzz_token_set_ratio\n",
      "fuzz_token_sort_ratio\n"
     ]
    }
   ],
   "source": [
    "data_train['len_q1'] = data_train.question1.apply(lambda x: len(str(x)))\n",
    "data_train['len_q2'] = data_train.question2.apply(lambda x: len(str(x)))\n",
    "data_train['diff_len'] = data_train.len_q1 - data_train.len_q2\n",
    "data_train['len_char_q1'] = data_train.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_train['len_char_q2'] = data_train.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_train['len_word_q1'] = data_train.question1.apply(lambda x: len(str(x).split()))\n",
    "data_train['len_word_q2'] = data_train.question2.apply(lambda x: len(str(x).split()))\n",
    "data_train['common_words'] = data_train.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection\\\n",
    "                                                            (set(str(x['question2']).lower().split()))), axis=1)\n",
    "data_train['prop_common_words'] = data_train.apply(lambda x: \\\n",
    "                                                   len(set(remove_punctuation(x['question1']).lower().split()).intersection\\\n",
    "                                                       (set(remove_punctuation(x['question2']).lower().split()))) / \\\n",
    "                                                   len(set(remove_punctuation(x['question1']).lower().split()).union\\\n",
    "                                                       (set(remove_punctuation(x['question2']).lower().split()))),axis=1 )\n",
    "\n",
    "\n",
    "data_train['fuzz_qratio'] = data_train.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_qratio')\n",
    "data_train['fuzz_WRatio'] = data_train.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_WRatio')\n",
    "data_train['fuzz_partial_ratio'] = data_train.apply(lambda x: fuzz.partial_ratio\\\n",
    "                                                    (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_partial_ratio')\n",
    "data_train['fuzz_partial_token_set_ratio'] = data_train.apply(lambda x: fuzz.partial_token_set_ratio\\\n",
    "                                                              (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_partial_token_set_ratio')\n",
    "data_train['fuzz_partial_token_sort_ratio'] = data_train.apply(lambda x: fuzz.partial_token_sort_ratio\\\n",
    "                                                               (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_partial_token_sort_ratio')\n",
    "data_train['fuzz_token_set_ratio'] = data_train.apply(lambda x: fuzz.token_set_ratio\\\n",
    "                                                      (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_token_set_ratio')\n",
    "data_train['fuzz_token_sort_ratio'] = data_train.apply(lambda x: fuzz.token_sort_ratio\\\n",
    "                                                       (str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "print('fuzz_token_sort_ratio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/seatgeek/fuzzywuzzy\n",
    "#https://pypi.python.org/pypi/fuzzywuzzy\n",
    "#http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>common_words</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>fuzz_WRatio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>fuzz_partial_token_set_ratio</th>\n",
       "      <th>fuzz_partial_token_sort_ratio</th>\n",
       "      <th>fuzz_qratio</th>\n",
       "      <th>fuzz_token_set_ratio</th>\n",
       "      <th>fuzz_token_sort_ratio</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>prop_common_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_duplicate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">0</th>\n",
       "      <th>count</th>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
<<<<<<< HEAD
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.960922</td>\n",
       "      <td>-0.849134</td>\n",
       "      <td>73.680136</td>\n",
       "      <td>60.334274</td>\n",
       "      <td>96.334270</td>\n",
       "      <td>63.232207</td>\n",
       "      <td>56.742929</td>\n",
       "      <td>67.718489</td>\n",
       "      <td>59.246060</td>\n",
       "      <td>20.679308</td>\n",
       "      <td>20.656158</td>\n",
       "      <td>63.455403</td>\n",
       "      <td>64.304536</td>\n",
       "      <td>11.582828</td>\n",
       "      <td>11.955730</td>\n",
       "      <td>0.300786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.199845</td>\n",
       "      <td>38.196508</td>\n",
       "      <td>16.832006</td>\n",
       "      <td>16.921124</td>\n",
       "      <td>14.094409</td>\n",
       "      <td>14.818205</td>\n",
       "      <td>18.296754</td>\n",
       "      <td>18.815512</td>\n",
       "      <td>16.806466</td>\n",
       "      <td>4.377631</td>\n",
       "      <td>4.584406</td>\n",
       "      <td>32.584157</td>\n",
       "      <td>38.116989</td>\n",
       "      <td>5.955508</td>\n",
       "      <td>7.162012</td>\n",
       "      <td>0.239106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1080.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>-14.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.121212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>487.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>623.000000</td>\n",
       "      <td>1169.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
=======
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.960922</td>\n",
       "      <td>-0.849134</td>\n",
       "      <td>73.680136</td>\n",
       "      <td>60.334274</td>\n",
       "      <td>96.334270</td>\n",
       "      <td>63.232207</td>\n",
       "      <td>56.742929</td>\n",
       "      <td>67.718489</td>\n",
       "      <td>59.246060</td>\n",
       "      <td>20.679308</td>\n",
       "      <td>20.656158</td>\n",
       "      <td>63.455403</td>\n",
       "      <td>64.304536</td>\n",
       "      <td>11.582828</td>\n",
       "      <td>11.955730</td>\n",
       "      <td>0.300786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.199845</td>\n",
       "      <td>38.196508</td>\n",
       "      <td>16.832006</td>\n",
       "      <td>16.921124</td>\n",
       "      <td>14.094409</td>\n",
       "      <td>14.818205</td>\n",
       "      <td>18.296754</td>\n",
       "      <td>18.815512</td>\n",
       "      <td>16.806466</td>\n",
       "      <td>4.377631</td>\n",
       "      <td>4.584406</td>\n",
       "      <td>32.584157</td>\n",
       "      <td>38.116989</td>\n",
       "      <td>5.955508</td>\n",
       "      <td>7.162012</td>\n",
       "      <td>0.239106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1080.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>-14.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.121212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>487.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>623.000000</td>\n",
       "      <td>1169.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
>>>>>>> parent of e831bcf... mais uma
       "      <th rowspan=\"8\" valign=\"top\">1</th>\n",
       "      <th>count</th>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
<<<<<<< HEAD
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.452436</td>\n",
       "      <td>-0.097586</td>\n",
       "      <td>81.319282</td>\n",
       "      <td>72.802382</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>74.420808</td>\n",
       "      <td>70.850197</td>\n",
       "      <td>82.662227</td>\n",
       "      <td>72.281450</td>\n",
       "      <td>19.451070</td>\n",
       "      <td>19.447010</td>\n",
       "      <td>52.841347</td>\n",
       "      <td>52.938933</td>\n",
       "      <td>9.847665</td>\n",
       "      <td>9.859999</td>\n",
       "      <td>0.466475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.666256</td>\n",
       "      <td>19.482448</td>\n",
       "      <td>10.609341</td>\n",
       "      <td>13.298586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.351533</td>\n",
       "      <td>14.729151</td>\n",
       "      <td>12.305596</td>\n",
       "      <td>13.509107</td>\n",
       "      <td>3.688251</td>\n",
       "      <td>3.660736</td>\n",
       "      <td>23.301917</td>\n",
       "      <td>23.285384</td>\n",
       "      <td>4.162756</td>\n",
       "      <td>4.155931</td>\n",
       "      <td>0.206193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-180.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
=======
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.452436</td>\n",
       "      <td>-0.097586</td>\n",
       "      <td>81.319282</td>\n",
       "      <td>72.802382</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>74.420808</td>\n",
       "      <td>70.850197</td>\n",
       "      <td>82.662227</td>\n",
       "      <td>72.281450</td>\n",
       "      <td>19.451070</td>\n",
       "      <td>19.447010</td>\n",
       "      <td>52.841347</td>\n",
       "      <td>52.938933</td>\n",
       "      <td>9.847665</td>\n",
       "      <td>9.859999</td>\n",
       "      <td>0.466475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.666256</td>\n",
       "      <td>19.482448</td>\n",
       "      <td>10.609341</td>\n",
       "      <td>13.298586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.351533</td>\n",
       "      <td>14.729151</td>\n",
       "      <td>12.305596</td>\n",
       "      <td>13.509107</td>\n",
       "      <td>3.688251</td>\n",
       "      <td>3.660736</td>\n",
       "      <td>23.301917</td>\n",
       "      <td>23.285384</td>\n",
       "      <td>4.162756</td>\n",
       "      <td>4.155931</td>\n",
       "      <td>0.206193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-180.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
>>>>>>> parent of e831bcf... mais uma
       "      <th>max</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>430.000000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     common_words       diff_len    fuzz_WRatio  \\\n",
       "is_duplicate                                                      \n",
       "0            count  255027.000000  255027.000000  255027.000000   \n",
       "             mean        3.960922      -0.849134      73.680136   \n",
       "             std         3.199845      38.196508      16.832006   \n",
       "             min         0.000000   -1080.000000       0.000000   \n",
       "             25%         2.000000     -14.000000      60.000000   \n",
       "             50%         3.000000       0.000000      83.000000   \n",
       "             75%         5.000000      14.000000      86.000000   \n",
       "             max        34.000000     487.000000     100.000000   \n",
       "1            count  149263.000000  149263.000000  149263.000000   \n",
       "             mean        5.452436      -0.097586      81.319282   \n",
       "             std         2.666256      19.482448      10.609341   \n",
       "             min         1.000000    -180.000000      38.000000   \n",
       "             25%         4.000000      -9.000000      74.000000   \n",
       "             50%         5.000000       0.000000      86.000000   \n",
       "             75%         7.000000       9.000000      88.000000   \n",
       "             max        41.000000     196.000000     100.000000   \n",
       "\n",
       "                    fuzz_partial_ratio  fuzz_partial_token_set_ratio  \\\n",
       "is_duplicate                                                           \n",
       "0            count       255027.000000                 255027.000000   \n",
       "             mean            60.334274                     96.334270   \n",
       "             std             16.921124                     14.094409   \n",
       "             min              0.000000                      0.000000   \n",
       "             25%             46.000000                    100.000000   \n",
       "             50%             57.000000                    100.000000   \n",
       "             75%             73.000000                    100.000000   \n",
       "             max            100.000000                    100.000000   \n",
       "1            count       149263.000000                 149263.000000   \n",
       "             mean            72.802382                    100.000000   \n",
       "             std             13.298586                      0.000000   \n",
       "             min             18.000000                    100.000000   \n",
       "             25%             63.000000                    100.000000   \n",
       "             50%             73.000000                    100.000000   \n",
       "             75%             83.000000                    100.000000   \n",
       "             max            100.000000                    100.000000   \n",
       "\n",
       "                    fuzz_partial_token_sort_ratio    fuzz_qratio  \\\n",
       "is_duplicate                                                       \n",
       "0            count                  255027.000000  255027.000000   \n",
       "             mean                       63.232207      56.742929   \n",
       "             std                        14.818205      18.296754   \n",
       "             min                         0.000000       0.000000   \n",
       "             25%                        52.000000      42.000000   \n",
       "             50%                        61.000000      52.000000   \n",
       "             75%                        74.000000      70.000000   \n",
       "             max                       100.000000     100.000000   \n",
       "1            count                  149263.000000  149263.000000   \n",
       "             mean                       74.420808      70.850197   \n",
       "             std                        12.351533      14.729151   \n",
       "             min                        30.000000      24.000000   \n",
       "             25%                        65.000000      60.000000   \n",
       "             50%                        74.000000      71.000000   \n",
       "             75%                        84.000000      82.000000   \n",
       "             max                       100.000000     100.000000   \n",
       "\n",
       "                    fuzz_token_set_ratio  fuzz_token_sort_ratio  \\\n",
       "is_duplicate                                                      \n",
       "0            count         255027.000000          255027.000000   \n",
       "             mean              67.718489              59.246060   \n",
       "             std               18.815512              16.806466   \n",
       "             min                0.000000               0.000000   \n",
       "             25%               54.000000              47.000000   \n",
       "             50%               67.000000              57.000000   \n",
       "             75%               83.000000              71.000000   \n",
       "             max              100.000000             100.000000   \n",
       "1            count         149263.000000          149263.000000   \n",
       "             mean              82.662227              72.281450   \n",
       "             std               12.305596              13.509107   \n",
       "             min               31.000000              26.000000   \n",
       "             25%               74.000000              62.000000   \n",
       "             50%               84.000000              72.000000   \n",
       "             75%               93.000000              83.000000   \n",
       "             max              100.000000             100.000000   \n",
<<<<<<< HEAD
       "\n",
       "                      len_char_q1    len_char_q2         len_q1  \\\n",
       "is_duplicate                                                      \n",
       "0            count  255027.000000  255027.000000  255027.000000   \n",
       "             mean       20.679308      20.656158      63.455403   \n",
       "             std         4.377631       4.584406      32.584157   \n",
       "             min         1.000000       1.000000       1.000000   \n",
       "             25%        18.000000      18.000000      41.000000   \n",
       "             50%        20.000000      20.000000      55.000000   \n",
       "             75%        23.000000      23.000000      78.000000   \n",
       "             max        52.000000      55.000000     623.000000   \n",
       "1            count  149263.000000  149263.000000  149263.000000   \n",
       "             mean       19.451070      19.447010      52.841347   \n",
       "             std         3.688251       3.660736      23.301917   \n",
       "             min         2.000000       1.000000       3.000000   \n",
       "             25%        17.000000      17.000000      37.000000   \n",
       "             50%        19.000000      19.000000      47.000000   \n",
       "             75%        22.000000      22.000000      62.000000   \n",
       "             max        47.000000      44.000000     430.000000   \n",
       "\n",
=======
       "\n",
       "                      len_char_q1    len_char_q2         len_q1  \\\n",
       "is_duplicate                                                      \n",
       "0            count  255027.000000  255027.000000  255027.000000   \n",
       "             mean       20.679308      20.656158      63.455403   \n",
       "             std         4.377631       4.584406      32.584157   \n",
       "             min         1.000000       1.000000       1.000000   \n",
       "             25%        18.000000      18.000000      41.000000   \n",
       "             50%        20.000000      20.000000      55.000000   \n",
       "             75%        23.000000      23.000000      78.000000   \n",
       "             max        52.000000      55.000000     623.000000   \n",
       "1            count  149263.000000  149263.000000  149263.000000   \n",
       "             mean       19.451070      19.447010      52.841347   \n",
       "             std         3.688251       3.660736      23.301917   \n",
       "             min         2.000000       1.000000       3.000000   \n",
       "             25%        17.000000      17.000000      37.000000   \n",
       "             50%        19.000000      19.000000      47.000000   \n",
       "             75%        22.000000      22.000000      62.000000   \n",
       "             max        47.000000      44.000000     430.000000   \n",
       "\n",
>>>>>>> parent of e831bcf... mais uma
       "                           len_q2    len_word_q1    len_word_q2  \\\n",
       "is_duplicate                                                      \n",
       "0            count  255027.000000  255027.000000  255027.000000   \n",
       "             mean       64.304536      11.582828      11.955730   \n",
       "             std        38.116989       5.955508       7.162012   \n",
       "             min         1.000000       1.000000       1.000000   \n",
       "             25%        40.000000       8.000000       8.000000   \n",
       "             50%        53.000000      10.000000      10.000000   \n",
       "             75%        78.000000      14.000000      14.000000   \n",
       "             max      1169.000000     125.000000     237.000000   \n",
       "1            count  149263.000000  149263.000000  149263.000000   \n",
       "             mean       52.938933       9.847665       9.859999   \n",
       "             std        23.285384       4.162756       4.155931   \n",
       "             min         2.000000       1.000000       1.000000   \n",
       "             25%        37.000000       7.000000       7.000000   \n",
       "             50%        47.000000       9.000000       9.000000   \n",
       "             75%        63.000000      11.000000      11.000000   \n",
       "             max       295.000000      80.000000      60.000000   \n",
       "\n",
       "                    prop_common_words  \n",
       "is_duplicate                           \n",
       "0            count      255027.000000  \n",
       "             mean            0.300786  \n",
       "             std             0.239106  \n",
       "             min             0.000000  \n",
       "             25%             0.121212  \n",
       "             50%             0.230769  \n",
       "             75%             0.428571  \n",
       "             max             1.000000  \n",
       "1            count      149263.000000  \n",
       "             mean            0.466475  \n",
       "             std             0.206193  \n",
       "             min             0.055556  \n",
       "             25%             0.307692  \n",
       "             50%             0.437500  \n",
       "             75%             0.600000  \n",
       "             max             1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.groupby('is_duplicate').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depois apagar as # para não ficar como comentário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data base of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving Pickle\n",
    "with open(os.path.join(save, 'train_features2.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train),f)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 13,
>>>>>>> parent of e831bcf... mais uma
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/joaomarcosest/Kaggle_Quora\n"
     ]
    }
   ],
   "source": [
    "# save features as CSV\n",
    "data_train.to_csv(os.path.join(save, 'train_features_2.csv'),index=False)\n",
    "print(save)\n",
<<<<<<< HEAD
    "\n",
=======
>>>>>>> parent of e831bcf... mais uma
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 14,
>>>>>>> parent of e831bcf... mais uma
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404290 entries, 0 to 404289\n",
      "Data columns (total 19 columns):\n",
      "question1                        404290 non-null object\n",
      "question2                        404288 non-null object\n",
      "is_duplicate                     404290 non-null int64\n",
      "len_q1                           404290 non-null int64\n",
      "len_q2                           404290 non-null int64\n",
      "diff_len                         404290 non-null int64\n",
      "len_char_q1                      404290 non-null int64\n",
      "len_char_q2                      404290 non-null int64\n",
      "len_word_q1                      404290 non-null int64\n",
      "len_word_q2                      404290 non-null int64\n",
      "common_words                     404290 non-null int64\n",
      "prop_common_words                404290 non-null float64\n",
      "fuzz_qratio                      404290 non-null int64\n",
      "fuzz_WRatio                      404290 non-null int64\n",
      "fuzz_partial_ratio               404290 non-null int64\n",
      "fuzz_partial_token_set_ratio     404290 non-null int64\n",
      "fuzz_partial_token_sort_ratio    404290 non-null int64\n",
      "fuzz_token_set_ratio             404290 non-null int64\n",
      "fuzz_token_sort_ratio            404290 non-null int64\n",
      "dtypes: float64(1), int64(16), object(2)\n",
      "memory usage: 58.6+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_train.info()\n",
    "\n",
    "#data_train_features = pd.read_csv(os.path.join(save, 'train_features.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlines_count = 0\\na=set()\\nfor item in range(0,404290):\\n    a =  a.union(\\n     set(remove_punctuation(data_train.question1[item]).    split()).union(set(remove_punctuation(data_train.question2[item]).split())))\\n    if item%50000==0:\\n        print(item)\\nprint(len(a))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "lines_count = 0\n",
    "a=set()\n",
    "for item in range(0,404290):\n",
    "    a =  a.union(\n",
    "     set(remove_punctuation(data_train.question1[item]).\\\n",
    "    split()).union(set(remove_punctuation(data_train.question2[item]).split())))\n",
    "    if item%50000==0:\n",
    "        print(item)\n",
    "print(len(a))\n",
    "'''\n",
    "#Resultado 136153 palavras única"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stemmers remove morphological affixes from words, leaving only the word stem.\n",
    "#http://www.nltk.org/howto/stem.html\n",
    "#The 'english' stemmer is better than the original 'porter' stemmer.\n",
    "#example; stemmer.stem('likely', 'bites') - like, bite \n",
    "#http://www.nltk.org/api/nltk.tokenize.html\n",
    "#A tokenizer that divides a string into substrings by splitting on the specified string (defined in subclasses).\n",
    "#word_tokenize(\"It's only a test\")- ['It', \"'s\", 'only', 'a', 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Editing questions with NLTK package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaning tool is used so you can easily choose which functions you want to use to clean te text\n",
    "def cleaning_tool(data, drop_na = True, lower_case = True, rm_duplicate = False, stopwords = False, \n",
    "                  punctuation = False, lemm = False, stem = False, list_of_stopwords = None):\n",
    "    \"\"\"\n",
    "    Function to process all data using calling functions from above, according to what was chosen.\n",
    "    :param data: data frame.\n",
    "    :param drop_na: If True drop all lines of data frame with NA\n",
    "    :param lower_case: If True transform for lower case\n",
    "    :param rm_duplicate: If True remove all duplicate words in questions\n",
    "    :param stopwords: If True removes stopwords\n",
    "    :param punctuation: If True removes punctuation\n",
    "    :param lemm: If True returns the phrase lemmatized\n",
    "    :param stem: If True returns the phrase stemmed\n",
    "    :param list_of_stopwords: List of stopwords to be used\n",
    "    :return: Question1 and Question2 processed according to parameters\n",
    "    \"\"\"\n",
    "    if drop_na == True:\n",
    "        data = data.dropna(0)\n",
    "    \n",
    "    if rm_duplicate == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_duplicate(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_duplicate(x))\n",
    "    \n",
    "    if lower_case == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: all_lower_case(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: all_lower_case(x))\n",
    "    \n",
    "    if stopwords == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_stopwords(x, list_of_stopwords))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_stopwords(x, list_of_stopwords))\n",
    "       \n",
    "    if punctuation == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_punctuation(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_punctuation(x))\n",
    "        \n",
    "    if lemm_wordnet == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: lemm_wordnet(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: lemm_wordnet(x))\n",
    "        \n",
    "    if stem_snowball == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: stem_snowball(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: stem_snowball(x))\n",
    "    \n",
    "    #We used it two times if some function create a new NA.\n",
    "    if drop_na == True:\n",
    "        data = data.dropna(0)    \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 25,
>>>>>>> parent of e831bcf... mais uma
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data_train_clean = cleaning_tool(data_train, stopwords=True, lemm=True,list_of_stopwords=mystopwords,punctuation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving Pickle\n",
    "with open(os.path.join(save, 'data_train_clean_features.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train_clean),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perguntar entre o split e word_tokenize\n",
    "#Verificar http://www.nltk.org/api/nltk.stem.html LEMATIZAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#Plain Word Counts\n",
    "#X_traincv_tf, X_testcv_tf, y_traincv_tf, y_testcv_tf = model_selection.train_test_split(train_data_features_tf,\n",
    "                                                                                        train[\"sentiment\"],\n",
    "                                                                                        test_size=0.2,\n",
    "                                                                                        random_state=0)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1=data_train_clean[0:200000]\n",
    "sample2=data_train_clean.sample(200000)\n",
    "teste1=data_train_clean[200001:]\n",
    "teste2=data_train_clean.sample(104287)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#teste1.sample(2)\n",
    "#teste2.sample(2)\n",
    "#sample1.sample(2)\n",
    "#sample2.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
=======
>>>>>>> parent of e831bcf... mais uma
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "#saving\n",
    "with open(os.path.join(save, 'datas_sample.pkl'),'wb') as f:\n",
    "    pickle.dump((sample1,\n",
    "                 sample2,\n",
    "                 teste1,\n",
    "                 teste2),f)"
=======
    "#Plain Word Counts\n",
    "#X_traincv_tf, X_testcv_tf, y_traincv_tf, y_testcv_tf = model_selection.train_test_split(train_data_features_tf,\n",
    "                                                                                        train[\"sentiment\"],\n",
    "                                                                                        test_size=0.2,\n",
    "                                                                                        random_state=0)"
>>>>>>> parent of e831bcf... mais uma
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading\n",
    "\n",
    "with open(os.path.join(save, 'datas_sample.pkl'),'rb') as f:\n",
    "    (sample1,\n",
    "     sample2,\n",
    "     teste1,\n",
    "     teste2) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
=======
   "execution_count": 24,
>>>>>>> parent of e831bcf... mais uma
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_tf = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 10000) \n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1_tf_question1 = vectorizer_tf.fit_transform(sample1.question1)\n",
    "sample1_tf_question1 = sample1_tf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample1_tf_question1_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample1_tf_question1 ,f)\n",
    "del(sample1_tf_question1)\n",
    "\n",
    "sample2_tf_question1 = vectorizer_tf.fit_transform(sample2.question1)\n",
    "sample2_tf_question1 = sample2_tf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample2_tf_question1_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample2_tf_question1 ,f)\n",
    "del(sample2_tf_question1)\n",
    "\n",
    "teste1_tf_question1 = vectorizer_tf.fit_transform(teste1.question1)\n",
    "teste1_tf_question1 = teste1_tf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste1_tf_question1_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(teste1_tf_question1 ,f)\n",
    "del(teste1_tf_question1)\n",
    "    \n",
    "sample1_tf_question1 = vectorizer_tf.fit_transform(teste2.question1)\n",
    "teste2_tf_question1 = teste2_tf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste2_tf_question1_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(teste2_tf_question1 ,f)\n",
    "del(teste2_tf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1_tf_question2 = vectorizer_tf.fit_transform(sample1.question2)\n",
    "sample1_tf_question2 = sample1_tf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample1_tf_question2_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample1_tf_question2 ,f)\n",
    "del(sample1_tf_question2)\n",
    "\n",
    "sample2_tf_question2 = vectorizer_tf.fit_transform(sample2.question2)\n",
    "sample2_tf_question2 = sample2_tf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample2_tf_question2_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample2_tf_question2 ,f)\n",
    "del(sample2_tf_question2)\n",
    "\n",
    "teste1_tf_question2 = vectorizer_tf.fit_transform(teste1.question2)\n",
    "teste1_tf_question2 = teste1_tf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste1_tf_question1_vector2.pkl'),'wb') as f:\n",
    "    pickle.dump(teste1_tf_question2 ,f)\n",
    "del(teste1_tf_question2)\n",
    "\n",
    "teste2_tf_question2 = vectorizer_tf.fit_transform(teste2.question2)\n",
    "teste2_tf_question2 = teste2_tf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste2_tf_question2_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(teste2_tf_question2 ,f)\n",
    "del(teste2_tf_question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 32,
>>>>>>> parent of e831bcf... mais uma
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "#Another approach using TfIDf vectorizer and using the texts with stopwords in:\n",
    "#https://github.com/zygmuntz/classifying-text/blob/master/bow_predict.py \n",
<<<<<<< HEAD
    "vectorizer_tfidf = TfidfVectorizer(analyzer='word', \\\n",
    "                                  preprocessor=None,\\\n",
    "                                  tokenizer=None,\\\n",
    "                                  stop_words=None,\\\n",
    "                                  max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1_tfidf_question1 = vectorizer_tfidf.fit_transform(sample1.question1)\n",
    "sample1_tfidf_question1 = sample1_tfidf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample1_tfidf_question1_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample1_tfidf_question1 ,f)\n",
    "del(sample1_tfidf_question1)\n",
    "\n",
    "sample2_tfidf_question1 = vectorizer_tfidf.fit_transform(sample2.question1)\n",
    "sample2_tfidf_question1 = sample2_tfidf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample2_tfidf_question1_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample2_tfidf_question1 ,f)\n",
    "del(sample2_tfidf_question1)\n",
    "\n",
    "teste1_tfidf_question1 = vectorizer_tfidf.fit_transform(teste1.question1)\n",
    "teste1_tfidf_question1 = teste1_tfidf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste1_tfidf_question1_vector2.pkl'),'wb') as f:\n",
    "    pickle.dump(teste1_tfidf_question1,f)\n",
    "del(teste1_tfidf_question1)\n",
    "\n",
    "teste2_tfidf_question1 = vectorizer_tfidf.fit_transform(teste2.question1)\n",
    "teste2_tfidf_question1 = teste2_tfidf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste2_tfidf_question1_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(teste2_tfidf_question1 ,f)\n",
    "del(teste2_tfidf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1_tfidf_question2 = vectorizer_tfidf.fit_transform(sample1.question2)\n",
    "sample1_tfidf_question2 = sample1_tfidf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample1_tfidf_question2_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample1_tfidf_question2 ,f)\n",
    "del(sample1_tfidf_question2)\n",
    "\n",
    "sample2_tfidf_question2 = vectorizer_tfidf.fit_transform(sample2.question2)\n",
    "sample2_tfidf_question2 = sample2_tfidf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample2_tfidf_question2_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample2_tfidf_question2 ,f)\n",
    "del(sample2_tfidf_question2)\n",
    "\n",
    "teste1_tfidf_question2 = vectorizer_tfidf.fit_transform(teste1.question2)\n",
    "teste1_tfidf_question2 = teste1_tfidf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste1_tfidf_question2_vector2.pkl'),'wb') as f:\n",
    "    pickle.dump(teste1_tfidf_question2,f)\n",
    "del(teste1_tfidf_question2)\n",
    "\n",
    "teste2_tfidf_question2 = vectorizer_tfidf.fit_transform(teste2.question2)\n",
    "teste2_tfidf_question2 = teste2_tfidf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste2_tfidf_question2_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(teste2_tfidf_question2 ,f)\n",
    "del(teste2_tfidf_question2)\n"
=======
    "vectorizer_tfidf = TfidfVectorizer(input='content',\n",
    "                                  #encoding='utf-8',\n",
    "                                  decode_error='strict',\n",
    "                                  strip_accents=None,\n",
    "                                  lowercase=True,\n",
    "                                  preprocessor=None,\n",
    "                                  tokenizer=None,\n",
    "                                  analyzer='word',\n",
    "                                  stop_words=None,\n",
    "                                  #token_pattern='(?u)\\b\\w\\w+\\b',\n",
    "                                  ngram_range=(1, 2),\n",
    "                                  max_df=1.0,\n",
    "                                  min_df=1,\n",
    "                                  max_features=5000,\n",
    "                                  vocabulary=None, \n",
    "                                  binary=False, \n",
    "                                  dtype=np.int64,\n",
    "                                  norm='l2',\n",
    "                                  use_idf=True,\n",
    "                                  smooth_idf=True,\n",
    "                                  sublinear_tf=True)"
>>>>>>> parent of e831bcf... mais uma
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(save, 'data_train_clean_features.pkl'),'rb') as f:\n",
    "    (data_train_clean) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_clean_tf_question1 = vectorizer_tf.fit_transform(data_train_clean.question1)\n",
    "# Numpy arrays are easy to work with\n",
    "data_train_clean_tf_question1 = data_train_clean_tf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'data_train_clean_tf_question1_vector3.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train_clean_tf_question1,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_clean_tf_question2 = vectorizer_tf.fit_transform(data_train_clean.question2)\n",
    "data_train_clean_tf_question2 = data_train_clean_tf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'data_train_clean_tf_question2_vector4.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train_clean_tf_question2,f)"
=======
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_clean_tf_question1 = vectorizer_tf.fit_transform(data_train_clean.question1)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "data_train_clean_tf_question1 = data_train_clean_tf.toarray()\n",
    "data_train_clean_tf_question2 = vectorizer_tf.fit_transform(data_train_clean.question2)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "data_train_clean_tf_question2 = data_train_clean_tf.toarray()\n"
>>>>>>> parent of e831bcf... mais uma
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404288, 10000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_clean_tfidf_question1 = vectorizer_tfidf.fit_transform(data_train_clean.question1)\n",
    "data_train_clean_tfidf_question1 = data_train_clean_tfidf.toarray()\n",
    "\n",
<<<<<<< HEAD
    "with open(os.path.join(save, 'data_train_clean_tfidf_question1_vector5.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train_clean_tfidf_question1,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
=======
>>>>>>> parent of e831bcf... mais uma
    "data_train_clean_tfidf_question2 = vectorizer_tfidf.fit_transform(data_train_clean.question1)\n",
    "data_train_clean_tfidf_question2 = data_train_clean_tfidf.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'data_train_clean_tfidf_question2_vector6.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train_clean_tfidf_question2,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_clean_tfidf_question2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save, 'data_train_features_vector2.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train_clean_tf_question1,\n",
    "                 data_train_clean_tf_question1,\n",
    "                 data_train_clean_tfidf_question1,\n",
    "                 data_train_clean_tfidf_question2),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''with open(os.path.join(outputs, 'data_train_features_vector2.pkl'),'rb') as f:\n",
    "    (train_data_features_tf, \n",
    "    test_data_features_tf,\n",
    "    train_data_features_tfidf,\n",
    "    test_data_features_tfidf) = pickle.load(f)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
