{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FGV EMAP\n",
    "\n",
    "## Modelagem e mineração de dados\n",
    "\n",
    "\n",
    "### Trabalho Kaggle Quora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alunos: Antonio Sombra e Joao Marcos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#strings\n",
    "import string\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "#Extras\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "#basic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import string\n",
    "import math\n",
    "\n",
    "\n",
    "# Visualisation\n",
    "import pylab\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>303503</th>\n",
       "      <td>What are some of the interesting encounters wi...</td>\n",
       "      <td>How is E &amp; I at BITS Pilani, Pilani campus?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157911</th>\n",
       "      <td>How high should one jump from to commit suicide?</td>\n",
       "      <td>How high can a cow jump?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338574</th>\n",
       "      <td>Do employees at Strategic Hotel &amp; Resorts have...</td>\n",
       "      <td>Do employees at Xenia Hotels &amp; Resorts have a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question1  \\\n",
       "303503  What are some of the interesting encounters wi...   \n",
       "157911   How high should one jump from to commit suicide?   \n",
       "338574  Do employees at Strategic Hotel & Resorts have...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "303503        How is E & I at BITS Pilani, Pilani campus?             0  \n",
       "157911                           How high can a cow jump?             0  \n",
       "338574  Do employees at Xenia Hotels & Resorts have a ...             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = \"/Dados/Kaggle/\"\n",
    "word2vec= \"/Dados/Word2vec\"\n",
    "save=  \"/home/joaomarcosest/Kaggle_Quora/Dados\"\n",
    "data_train = pd.read_csv(os.path.join(datapath, 'train.csv'))\n",
    "data_train=data_train.drop(['id','qid1','qid2'],axis=1)\n",
    "data_train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404290 entries, 0 to 404289\n",
      "Data columns (total 3 columns):\n",
      "question1       404290 non-null object\n",
      "question2       404288 non-null object\n",
      "is_duplicate    404290 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 9.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404290.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.369198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.482588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_duplicate\n",
       "count  404290.000000\n",
       "mean        0.369198\n",
       "std         0.482588\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         1.000000\n",
       "max         1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.info()\n",
    "data_train.describe()\n",
    "\n",
    "#0.369198 percente is duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['question1', 'question2', 'is_duplicate'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_train.columns)\n",
    "type(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choosing the list of stopwords\n",
    "mystopwords = nltk.corpus.stopwords.words('english')\n",
    "list_of_words= ['where','what','when','why','between','who','how','which']\n",
    "for item in list_of_words:\n",
    "        mystopwords.remove(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the step by step guid to invest in share market in india?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(phrase,list_stopwords):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes all stopwords from a list\n",
    "    :param phrase: String. A phrase.\n",
    "    :param list_stopwords: List. A list of stopwords\n",
    "    :return: The same phrase without stopwords\n",
    "    \"\"\"\n",
    "    final_phrase = []\n",
    "    words = phrase.split(\" \")\n",
    "    for word in words:\n",
    "        if word not in list_stopwords:\n",
    "            final_phrase.append((word))\n",
    "    \n",
    "    final_phrase = ' '.join(final_phrase)\n",
    "    \n",
    "    return final_phrase\n",
    "    \n",
    "def remove_punctuation(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes all punctuation from it\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase without punctuation\n",
    "    \"\"\"\n",
    "    #https://www.tutorialspoint.com/python/string_maketrans.htm\n",
    "    #Check if NA\n",
    "    if type(phrase) is float:\n",
    "        if math.isnan(phrase):\n",
    "            return (\"\")\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    phrase = phrase.translate(translator) #removing punctuation\n",
    "        \n",
    "    return phrase\n",
    "\n",
    "def lemm_wordnet(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes lemmatizes it\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase in lemmas\n",
    "    \"\"\"\n",
    "    lemm = WordNetLemmatizer()\n",
    "    \n",
    "    #NA is a float type, so this if is to avoid conflict\n",
    "    if type(phrase) is not float:\n",
    "        phrase = [lemm.lemmatize(i) for i in phrase.split()]\n",
    "        phrase = ' '.join(phrase)\n",
    "    else:\n",
    "        return \"\"\n",
    "    return phrase\n",
    "    \n",
    "def remove_duplicate(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes all duplicate words\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase with just unique words\n",
    "    \"\"\"\n",
    "    aux_phrase = []\n",
    "        \n",
    "    if type(phrase) is not float:\n",
    "        \n",
    "        for i in phrase.split():\n",
    "            \n",
    "            if i not in aux_phrase:\n",
    "                aux_phrase.append(i)\n",
    "    \n",
    "    phrase = ' '.join(aux_phrase)\n",
    "    \n",
    "    return phrase\n",
    "    \n",
    "    \n",
    "def all_lower_case(phrase):    \n",
    "    \"\"\"\n",
    "    Receives a phrase and makes it lower case\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase in lower case\n",
    "    \"\"\"\n",
    "    if type(phrase) is not float:\n",
    "            phrase = phrase.lower()\n",
    "    return phrase\n",
    "    \n",
    "def stem_snowball(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and returns the same phrase stemmed, lowercase phrase without stopwords\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: String. Stemmed, lowercase phrase without stopwords\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    #Stem words according to stemmer\n",
    "    final_phrase = []\n",
    "    words = phrase.split(\" \")\n",
    "    for word in words:\n",
    "        final_phrase.append((stemmer.stem(word)))\n",
    "    \n",
    "    final_phrase = ' '.join(final_phrase)\n",
    "    \n",
    "    return final_phrase\n",
    "\n",
    "stem_snowball(\"What is the step by step guide to invest in share market in india?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuzz_qratio\n",
      "fuzz_WRatio\n",
      "fuzz_partial_ratio\n",
      "fuzz_partial_token_set_ratio\n",
      "fuzz_partial_token_sort_ratio\n",
      "fuzz_token_set_ratio\n",
      "fuzz_token_sort_ratio\n"
     ]
    }
   ],
   "source": [
    "data_train['len_q1'] = data_train.question1.apply(lambda x: len(str(x)))\n",
    "data_train['len_q2'] = data_train.question2.apply(lambda x: len(str(x)))\n",
    "data_train['diff_len'] = data_train.len_q1 - data_train.len_q2\n",
    "data_train['len_char_q1'] = data_train.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_train['len_char_q2'] = data_train.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_train['len_word_q1'] = data_train.question1.apply(lambda x: len(str(x).split()))\n",
    "data_train['len_word_q2'] = data_train.question2.apply(lambda x: len(str(x).split()))\n",
    "data_train['common_words'] = data_train.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection\\\n",
    "                                                            (set(str(x['question2']).lower().split()))), axis=1)\n",
    "\n",
    "data_train['prop_common_words'] = data_train.apply(lambda x: \\\n",
    "                                                   len(set(remove_punctuation(x['question1']).lower().split()).intersection\\\n",
    "                                                       (set(remove_punctuation(x['question2']).lower().split()))) / \\\n",
    "                                                   len(set(remove_punctuation(x['question1']).lower().split()).union\\\n",
    "                                                       (set(remove_punctuation(x['question2']).lower().split()))),axis=1 )\n",
    "\n",
    "\n",
    "data_train['fuzz_qratio'] = data_train.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_qratio')\n",
    "\n",
    "data_train['fuzz_WRatio'] = data_train.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_WRatio')\n",
    "\n",
    "data_train['fuzz_partial_ratio'] = data_train.apply(lambda x: fuzz.partial_ratio\\\n",
    "                                                    (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_partial_ratio')\n",
    "\n",
    "data_train['fuzz_partial_token_set_ratio'] = data_train.apply(lambda x: fuzz.partial_token_set_ratio\\\n",
    "                                                              (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_partial_token_set_ratio')\n",
    "\n",
    "data_train['fuzz_partial_token_sort_ratio'] = data_train.apply(lambda x: fuzz.partial_token_sort_ratio\\\n",
    "                                                               (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_partial_token_sort_ratio')\n",
    "\n",
    "data_train['fuzz_token_set_ratio'] = data_train.apply(lambda x: fuzz.token_set_ratio\\\n",
    "                                                      (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_token_set_ratio')\n",
    "\n",
    "data_train['fuzz_token_sort_ratio'] = data_train.apply(lambda x: fuzz.token_sort_ratio\\\n",
    "                                                       (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_token_sort_ratio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/seatgeek/fuzzywuzzy\n",
    "#https://pypi.python.org/pypi/fuzzywuzzy\n",
    "#http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>common_words</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>fuzz_WRatio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>fuzz_partial_token_set_ratio</th>\n",
       "      <th>fuzz_partial_token_sort_ratio</th>\n",
       "      <th>fuzz_qratio</th>\n",
       "      <th>fuzz_token_set_ratio</th>\n",
       "      <th>fuzz_token_sort_ratio</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>prop_common_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_duplicate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">0</th>\n",
       "      <th>count</th>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "      <td>255027.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.960922</td>\n",
       "      <td>-0.849134</td>\n",
       "      <td>73.680136</td>\n",
       "      <td>60.334274</td>\n",
       "      <td>96.334270</td>\n",
       "      <td>63.232207</td>\n",
       "      <td>56.742929</td>\n",
       "      <td>67.718489</td>\n",
       "      <td>59.246060</td>\n",
       "      <td>20.679308</td>\n",
       "      <td>20.656158</td>\n",
       "      <td>63.455403</td>\n",
       "      <td>64.304536</td>\n",
       "      <td>11.582828</td>\n",
       "      <td>11.955730</td>\n",
       "      <td>0.300786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.199845</td>\n",
       "      <td>38.196508</td>\n",
       "      <td>16.832006</td>\n",
       "      <td>16.921124</td>\n",
       "      <td>14.094409</td>\n",
       "      <td>14.818205</td>\n",
       "      <td>18.296754</td>\n",
       "      <td>18.815512</td>\n",
       "      <td>16.806466</td>\n",
       "      <td>4.377631</td>\n",
       "      <td>4.584406</td>\n",
       "      <td>32.584157</td>\n",
       "      <td>38.116989</td>\n",
       "      <td>5.955508</td>\n",
       "      <td>7.162012</td>\n",
       "      <td>0.239106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1080.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>-14.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.121212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>487.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>623.000000</td>\n",
       "      <td>1169.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">1</th>\n",
       "      <th>count</th>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "      <td>149263.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.452436</td>\n",
       "      <td>-0.097586</td>\n",
       "      <td>81.319282</td>\n",
       "      <td>72.802382</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>74.420808</td>\n",
       "      <td>70.850197</td>\n",
       "      <td>82.662227</td>\n",
       "      <td>72.281450</td>\n",
       "      <td>19.451070</td>\n",
       "      <td>19.447010</td>\n",
       "      <td>52.841347</td>\n",
       "      <td>52.938933</td>\n",
       "      <td>9.847665</td>\n",
       "      <td>9.859999</td>\n",
       "      <td>0.466475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.666256</td>\n",
       "      <td>19.482448</td>\n",
       "      <td>10.609341</td>\n",
       "      <td>13.298586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.351533</td>\n",
       "      <td>14.729151</td>\n",
       "      <td>12.305596</td>\n",
       "      <td>13.509107</td>\n",
       "      <td>3.688251</td>\n",
       "      <td>3.660736</td>\n",
       "      <td>23.301917</td>\n",
       "      <td>23.285384</td>\n",
       "      <td>4.162756</td>\n",
       "      <td>4.155931</td>\n",
       "      <td>0.206193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-180.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>430.000000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     common_words       diff_len    fuzz_WRatio  \\\n",
       "is_duplicate                                                      \n",
       "0            count  255027.000000  255027.000000  255027.000000   \n",
       "             mean        3.960922      -0.849134      73.680136   \n",
       "             std         3.199845      38.196508      16.832006   \n",
       "             min         0.000000   -1080.000000       0.000000   \n",
       "             25%         2.000000     -14.000000      60.000000   \n",
       "             50%         3.000000       0.000000      83.000000   \n",
       "             75%         5.000000      14.000000      86.000000   \n",
       "             max        34.000000     487.000000     100.000000   \n",
       "1            count  149263.000000  149263.000000  149263.000000   \n",
       "             mean        5.452436      -0.097586      81.319282   \n",
       "             std         2.666256      19.482448      10.609341   \n",
       "             min         1.000000    -180.000000      38.000000   \n",
       "             25%         4.000000      -9.000000      74.000000   \n",
       "             50%         5.000000       0.000000      86.000000   \n",
       "             75%         7.000000       9.000000      88.000000   \n",
       "             max        41.000000     196.000000     100.000000   \n",
       "\n",
       "                    fuzz_partial_ratio  fuzz_partial_token_set_ratio  \\\n",
       "is_duplicate                                                           \n",
       "0            count       255027.000000                 255027.000000   \n",
       "             mean            60.334274                     96.334270   \n",
       "             std             16.921124                     14.094409   \n",
       "             min              0.000000                      0.000000   \n",
       "             25%             46.000000                    100.000000   \n",
       "             50%             57.000000                    100.000000   \n",
       "             75%             73.000000                    100.000000   \n",
       "             max            100.000000                    100.000000   \n",
       "1            count       149263.000000                 149263.000000   \n",
       "             mean            72.802382                    100.000000   \n",
       "             std             13.298586                      0.000000   \n",
       "             min             18.000000                    100.000000   \n",
       "             25%             63.000000                    100.000000   \n",
       "             50%             73.000000                    100.000000   \n",
       "             75%             83.000000                    100.000000   \n",
       "             max            100.000000                    100.000000   \n",
       "\n",
       "                    fuzz_partial_token_sort_ratio    fuzz_qratio  \\\n",
       "is_duplicate                                                       \n",
       "0            count                  255027.000000  255027.000000   \n",
       "             mean                       63.232207      56.742929   \n",
       "             std                        14.818205      18.296754   \n",
       "             min                         0.000000       0.000000   \n",
       "             25%                        52.000000      42.000000   \n",
       "             50%                        61.000000      52.000000   \n",
       "             75%                        74.000000      70.000000   \n",
       "             max                       100.000000     100.000000   \n",
       "1            count                  149263.000000  149263.000000   \n",
       "             mean                       74.420808      70.850197   \n",
       "             std                        12.351533      14.729151   \n",
       "             min                        30.000000      24.000000   \n",
       "             25%                        65.000000      60.000000   \n",
       "             50%                        74.000000      71.000000   \n",
       "             75%                        84.000000      82.000000   \n",
       "             max                       100.000000     100.000000   \n",
       "\n",
       "                    fuzz_token_set_ratio  fuzz_token_sort_ratio  \\\n",
       "is_duplicate                                                      \n",
       "0            count         255027.000000          255027.000000   \n",
       "             mean              67.718489              59.246060   \n",
       "             std               18.815512              16.806466   \n",
       "             min                0.000000               0.000000   \n",
       "             25%               54.000000              47.000000   \n",
       "             50%               67.000000              57.000000   \n",
       "             75%               83.000000              71.000000   \n",
       "             max              100.000000             100.000000   \n",
       "1            count         149263.000000          149263.000000   \n",
       "             mean              82.662227              72.281450   \n",
       "             std               12.305596              13.509107   \n",
       "             min               31.000000              26.000000   \n",
       "             25%               74.000000              62.000000   \n",
       "             50%               84.000000              72.000000   \n",
       "             75%               93.000000              83.000000   \n",
       "             max              100.000000             100.000000   \n",
       "\n",
       "                      len_char_q1    len_char_q2         len_q1  \\\n",
       "is_duplicate                                                      \n",
       "0            count  255027.000000  255027.000000  255027.000000   \n",
       "             mean       20.679308      20.656158      63.455403   \n",
       "             std         4.377631       4.584406      32.584157   \n",
       "             min         1.000000       1.000000       1.000000   \n",
       "             25%        18.000000      18.000000      41.000000   \n",
       "             50%        20.000000      20.000000      55.000000   \n",
       "             75%        23.000000      23.000000      78.000000   \n",
       "             max        52.000000      55.000000     623.000000   \n",
       "1            count  149263.000000  149263.000000  149263.000000   \n",
       "             mean       19.451070      19.447010      52.841347   \n",
       "             std         3.688251       3.660736      23.301917   \n",
       "             min         2.000000       1.000000       3.000000   \n",
       "             25%        17.000000      17.000000      37.000000   \n",
       "             50%        19.000000      19.000000      47.000000   \n",
       "             75%        22.000000      22.000000      62.000000   \n",
       "             max        47.000000      44.000000     430.000000   \n",
       "\n",
       "                           len_q2    len_word_q1    len_word_q2  \\\n",
       "is_duplicate                                                      \n",
       "0            count  255027.000000  255027.000000  255027.000000   \n",
       "             mean       64.304536      11.582828      11.955730   \n",
       "             std        38.116989       5.955508       7.162012   \n",
       "             min         1.000000       1.000000       1.000000   \n",
       "             25%        40.000000       8.000000       8.000000   \n",
       "             50%        53.000000      10.000000      10.000000   \n",
       "             75%        78.000000      14.000000      14.000000   \n",
       "             max      1169.000000     125.000000     237.000000   \n",
       "1            count  149263.000000  149263.000000  149263.000000   \n",
       "             mean       52.938933       9.847665       9.859999   \n",
       "             std        23.285384       4.162756       4.155931   \n",
       "             min         2.000000       1.000000       1.000000   \n",
       "             25%        37.000000       7.000000       7.000000   \n",
       "             50%        47.000000       9.000000       9.000000   \n",
       "             75%        63.000000      11.000000      11.000000   \n",
       "             max       295.000000      80.000000      60.000000   \n",
       "\n",
       "                    prop_common_words  \n",
       "is_duplicate                           \n",
       "0            count      255027.000000  \n",
       "             mean            0.300786  \n",
       "             std             0.239106  \n",
       "             min             0.000000  \n",
       "             25%             0.121212  \n",
       "             50%             0.230769  \n",
       "             75%             0.428571  \n",
       "             max             1.000000  \n",
       "1            count      149263.000000  \n",
       "             mean            0.466475  \n",
       "             std             0.206193  \n",
       "             min             0.055556  \n",
       "             25%             0.307692  \n",
       "             50%             0.437500  \n",
       "             75%             0.600000  \n",
       "             max             1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.groupby('is_duplicate').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data base of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404290 entries, 0 to 404289\n",
      "Data columns (total 19 columns):\n",
      "question1                        404290 non-null object\n",
      "question2                        404288 non-null object\n",
      "is_duplicate                     404290 non-null int64\n",
      "len_q1                           404290 non-null int64\n",
      "len_q2                           404290 non-null int64\n",
      "diff_len                         404290 non-null int64\n",
      "len_char_q1                      404290 non-null int64\n",
      "len_char_q2                      404290 non-null int64\n",
      "len_word_q1                      404290 non-null int64\n",
      "len_word_q2                      404290 non-null int64\n",
      "common_words                     404290 non-null int64\n",
      "prop_common_words                404290 non-null float64\n",
      "fuzz_qratio                      404290 non-null int64\n",
      "fuzz_WRatio                      404290 non-null int64\n",
      "fuzz_partial_ratio               404290 non-null int64\n",
      "fuzz_partial_token_set_ratio     404290 non-null int64\n",
      "fuzz_partial_token_sort_ratio    404290 non-null int64\n",
      "fuzz_token_set_ratio             404290 non-null int64\n",
      "fuzz_token_sort_ratio            404290 non-null int64\n",
      "dtypes: float64(1), int64(16), object(2)\n",
      "memory usage: 58.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Pickle\n",
    "with open(os.path.join(save, 'train_features.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save features as CSV\n",
    "data_train.to_csv(os.path.join(save, 'train_features.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlines_count = 0\\na=set()\\nfor item in range(0,404290):\\n    a =  a.union(\\n     set(remove_punctuation(data_train.question1[item]).    split()).union(set(remove_punctuation(data_train.question2[item]).split())))\\n    if item%50000==0:\\n        print(item)\\nprint(len(a))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count the number of words in data\n",
    "'''\n",
    "lines_count = 0\n",
    "a=set()\n",
    "for item in range(0,404290):\n",
    "    a =  a.union(\n",
    "     set(remove_punctuation(data_train.question1[item]).\\\n",
    "    split()).union(set(remove_punctuation(data_train.question2[item]).split())))\n",
    "    if item%50000==0:\n",
    "        print(item)\n",
    "print(len(a))\n",
    "'''\n",
    "#Resultado 136153 palavras única"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stemmers remove morphological affixes from words, leaving only the word stem.\n",
    "#http://www.nltk.org/howto/stem.html\n",
    "#The 'english' stemmer is better than the original 'porter' stemmer.\n",
    "#example; stemmer.stem('likely', 'bites') - like, bite \n",
    "#http://www.nltk.org/api/nltk.tokenize.html\n",
    "#A tokenizer that divides a string into substrings by splitting on the specified string (defined in subclasses).\n",
    "#word_tokenize(\"It's only a test\")- ['It', \"'s\", 'only', 'a', 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Clearing data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaning tool is used so you can easily choose which functions you want to use to clean te text\n",
    "def cleaning_tool(data, drop_na = True, lower_case = True, rm_duplicate = False, stopwords = False, \n",
    "                  punctuation = False, lemm = False, stem = False, list_of_stopwords = None):\n",
    "    \"\"\"\n",
    "    Function to process all data using calling functions from above, according to what was chosen.\n",
    "    :param data: data frame.\n",
    "    :param drop_na: If True drop all lines of data frame with NA\n",
    "    :param lower_case: If True transform for lower case\n",
    "    :param rm_duplicate: If True remove all duplicate words in questions\n",
    "    :param stopwords: If True removes stopwords\n",
    "    :param punctuation: If True removes punctuation\n",
    "    :param lemm: If True returns the phrase lemmatized\n",
    "    :param stem: If True returns the phrase stemmed\n",
    "    :param list_of_stopwords: List of stopwords to be used\n",
    "    :return: Question1 and Question2 processed according to parameters\n",
    "    \"\"\"\n",
    "    if drop_na == True:\n",
    "        data = data.dropna(0)\n",
    "    \n",
    "    if rm_duplicate == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_duplicate(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_duplicate(x))\n",
    "    \n",
    "    if lower_case == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: all_lower_case(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: all_lower_case(x))\n",
    "    \n",
    "    if stopwords == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_stopwords(x, list_of_stopwords))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_stopwords(x, list_of_stopwords))\n",
    "       \n",
    "    if punctuation == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_punctuation(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_punctuation(x))\n",
    "        \n",
    "    if lemm_wordnet == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: lemm_wordnet(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: lemm_wordnet(x))\n",
    "        \n",
    "    if stem_snowball == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: stem_snowball(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: stem_snowball(x))\n",
    "    \n",
    "    #We used it two times if some function create a new NA.\n",
    "    if drop_na == True:\n",
    "        data = data.dropna(0)    \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data_train_clean = cleaning_tool(data_train, stopwords=True, lemm=True,list_of_stopwords=mystopwords,punctuation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving Pickle\n",
    "with open(os.path.join(save, 'data_train_clean_features.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train_clean),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Plain Word Counts\\n#X_traincv_tf, X_testcv_tf, y_traincv_tf, y_testcv_tf = model_selection.train_test_split(train_data_features_tf,\\n                                                                                        train[\"sentiment\"],\\n                                                                                        test_size=0.2,\\n                                                                                        random_state=0)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Plain Word Counts\n",
    "#X_traincv_tf, X_testcv_tf, y_traincv_tf, y_testcv_tf = model_selection.train_test_split(train_data_features_tf,\n",
    "                                                                                        train[\"sentiment\"],\n",
    "                                                                                        test_size=0.2,\n",
    "                                                                                        random_state=0)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1=data_train_clean[0:300000]\n",
    "sample2=data_train_clean.sample(300000)\n",
    "teste1=data_train_clean[300001:]\n",
    "#teste2=data_train_clean.sample(104287)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#teste1.sample(2)\n",
    "#teste2.sample(2)\n",
    "#sample1.sample(2)\n",
    "#sample2.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open(os.path.join(save, 'datas_sample.pkl'),'wb') as f:\n",
    "    pickle.dump((sample1,\n",
    "                 sample2,\n",
    "                 teste1),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_tf = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 10000) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_TF data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = data_train_clean.question1.append([data_train_clean.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-bd80586f8bc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_train_clean_tf_question1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector_fitt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Numpy arrays are easy to work with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata_train_clean_tf_question1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_train_clean_tf_question1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'data_train_clean_tf_question1.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;31m##############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__numpy_ufunc__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vector_fitt = vectorizer_tf.fit(questions)\n",
    "data_train_clean_tf_question1 = vector_fitt.transform(data_train_clean.question1)\n",
    "# Numpy arrays are easy to work with\n",
    "data_train_clean_tf_question1 = data_train_clean_tf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save,'data_train_clean_tf_question1.pkl'),'wb') as f:\n",
    "    pickle.dump(data_train_clean_tf_question1,f)\n",
    "del(data_train_clean_tf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_clean_tf_question2 = vector_fitt.transform(data_train_clean.question2)\n",
    " # Numpy arrays are easy to work with\n",
    "data_train_clean_tf_question2 = data_train_clean_tf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'data_train_clean_tf_question2.pkl'),'wb') as f:\n",
    "    pickle.dump(data_train_clean_tf_question2,f)\n",
    "del(data_train_clean_tf_question2)\n",
    "del(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_TF sample1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1_questions=sample1.question1.append([sample1.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tf.fit(sample1_questions)\n",
    "sample1_tf_question1 = vector_fitt.transform(sample1.question1)\n",
    "# Numpy arrays are easy to work with\n",
    "sample1_tf_question1 = sample1_tf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample1_tf_question1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample1_tf_question1,f)\n",
    "del(sample1_tf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1_tf_question2 = vector_fitt.transform(sample1.question2)\n",
    "sample1_tf_question2 = sample1_tf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample1_tf_question2.pkl'),'wb') as f:\n",
    "    pickle.dump(sample1_tf_question2,f)\n",
    "del(sample1_tf_question2)\n",
    "del(sample1_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_TF sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample2_questions=sample2.question1.append([sample1.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tf.fit(sample2_questions)\n",
    "sample2_tf_question1 = vector_fitt.transform(sample2.question1)\n",
    "sample2_tf_question1 = sample2_tf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample2_tf_question1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample2_tf_question1,f)\n",
    "del(sample2_tf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2_tf_question2 = vector_fitt.transform(sample2.question2)\n",
    "sample2_tf_question2 = sample2_tf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample2_tf_question2.pkl'),'wb') as f:\n",
    "    pickle.dump(sample2_tf_question2,f)\n",
    "del(sample2_tf_question2)\n",
    "del(sample2_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_TF teste1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teste1_questions=teste1.question1.append([teste1.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tf.fit(teste1_questions)\n",
    "teste1_tf_question1 = vector_fitt.transform(teste1.question1)\n",
    "teste1_tf_question1 = teste1_tf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste1_tf_question1.pkl'),'wb') as f:\n",
    "    pickle.dump(teste1_tf_question1,f)\n",
    "del(teste1_tf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tf.fit(teste1_questions)\n",
    "teste1_tf_question2 = vector_fitt.transform(teste1.question2)\n",
    "teste1_tf_question2 = teste1_tf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste1_tf_question2.pkl'),'wb') as f:\n",
    "    pickle.dump(teste1_tf_question2,f)\n",
    "del(teste1_tf_question2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "#Another approach using TfIDf vectorizer and using the texts with stopwords in:\n",
    "#https://github.com/zygmuntz/classifying-text/blob/master/bow_predict.py \n",
    "vectorizer_tfidf = TfidfVectorizer(analyzer='word', \\\n",
    "                                  preprocessor=None,\\\n",
    "                                  tokenizer=None,\\\n",
    "                                  stop_words=None,\\\n",
    "                                  max_features=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_TFIDF data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data_train_clean.question1.append([data_train_clean.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tfidf.fit(questions)\n",
    "data_train_clean_tfidf_question1 = vector_fitt.transform(data_train_clean.question1)\n",
    "# Numpy arrays are easy to work with\n",
    "data_train_clean_tfidf_question1 = data_train_clean_tfidf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save,'data_train_clean_tfidf_question1.pkl'),'wb') as f:\n",
    "    pickle.dump(data_train_clean_tfidf_question1,f)\n",
    "del(data_train_clean_tfidf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_clean_tfidf_question2 = vector_fitt.transform(data_train_clean.question2)\n",
    " # Numpy arrays are easy to work with\n",
    "data_train_clean_tfidf_question2 = data_train_clean_tfidf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'data_train_clean_tfidf_question2.pkl'),'wb') as f:\n",
    "    pickle.dump(data_train_clean_tfidf_question2,f)\n",
    "del(data_train_clean_tfidf_question2)\n",
    "del(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_tfidf sample1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1_questions=sample1.question1.append([sample1.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tfidf.fit(sample1_questions)\n",
    "sample1_tfidf_question1 = vector_fitt.transform(sample1.question1)\n",
    "# Numpy arrays are easy to work with\n",
    "sample1_tfidf_question1 = sample1_tfidf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample1_tfidf_question1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample1_tfidf_question1,f)\n",
    "del(sample1_tfidf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1_tfidf_question2 = vector_fitt.transform(sample1.question2)\n",
    "sample1_tfidf_question2 = sample1_tfidf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample1_tfidf_question2.pkl'),'wb') as f:\n",
    "    pickle.dump(sample1_tfidf_question2,f)\n",
    "del(sample1_tfidf_question2)\n",
    "del(sample1_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_tfidf sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample2_questions=sample2.question1.append([sample1.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tfidf.fit(sample2_questions)\n",
    "sample2_tfidf_question1 = vector_fitt.transform(sample2.question1)\n",
    "sample2_tfidf_question1 = sample2_tfidf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample2_tfidf_question1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample2_tfidf_question1,f)\n",
    "del(sample2_tfidf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample2_tfidf_question2 = vector_fitt.transform(sample2.question2)\n",
    "sample2_tfidf_question2 = sample2_tfidf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample2_tfidf_question2.pkl'),'wb') as f:\n",
    "    pickle.dump(sample2_tfidf_question2,f)\n",
    "del(sample2_tfidf_question2)\n",
    "del(sample2_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_tfidf teste1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teste1_questions=teste1.question1.append([teste1.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tfidf.fit(teste1_questions)\n",
    "teste1_tfidf_question1 = vector_fitt.transform(teste1.question1)\n",
    "teste1_tfidf_question1 = teste1_tfidf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste1_tfidf_question1.pkl'),'wb') as f:\n",
    "    pickle.dump(teste1_tfidf_question1,f)\n",
    "del(teste1_tfidf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tfidf.fit(teste1_questions)\n",
    "teste1_tfidf_question2 = vector_fitt.transform(teste1.question2)\n",
    "teste1_tfidf_question2 = teste1_tfidf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste1_tfidf_question2.pkl'),'wb') as f:\n",
    "    pickle.dump(teste1_tfidf_question2,f)\n",
    "del(teste1_tfidf_question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''with open(os.path.join(outputs, 'data_train_features_vector2.pkl'),'rb') as f:\n",
    "    (train_data_features_tf, \n",
    "    test_data_features_tf,\n",
    "    train_data_features_tfidf,\n",
    "    test_data_features_tfidf) = pickle.load(f)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
