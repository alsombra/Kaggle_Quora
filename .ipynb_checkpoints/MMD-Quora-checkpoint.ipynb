{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FGV EMAP\n",
    "\n",
    "## Modelagem e mineração de dados\n",
    "\n",
    "\n",
    "### Trabalho Kaggle Quora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alunos: Antonio Sombra e Joao Marcos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#strings\n",
    "import string\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "#Extras\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "#basic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import string\n",
    "import math\n",
    "\n",
    "\n",
    "# Visualisation\n",
    "import pylab\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"/Dados/Kaggle/\"\n",
    "word2vec= \"/Dados/Word2vec\"\n",
    "save=  os.getcwd()\n",
    "data_train = pd.read_csv(os.path.join(datapath, 'train.csv'))\n",
    "data_train=data_train.drop(['id','qid1','qid2'],axis=1)\n",
    "data_train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.info()\n",
    "data_train.describe()\n",
    "\n",
    "#0.369198 percente is duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train.columns)\n",
    "type(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choosing the list of stopwords\n",
    "mystopwords = nltk.corpus.stopwords.words('english')\n",
    "list_of_words= ['where','what','when','why','between','who','how','which']\n",
    "for item in list_of_words:\n",
    "        mystopwords.remove(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing questions with NLTK package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(phrase,list_stopwords):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes all stopwords from a list\n",
    "    :param phrase: String. A phrase.\n",
    "    :param list_stopwords: List. A list of stopwords\n",
    "    :return: The same phrase without stopwords\n",
    "    \"\"\"\n",
    "    final_phrase = []\n",
    "    words = phrase.split(\" \")\n",
    "    for word in words:\n",
    "        if word not in list_stopwords:\n",
    "            final_phrase.append((word))\n",
    "    \n",
    "    final_phrase = ' '.join(final_phrase)\n",
    "    \n",
    "    return final_phrase\n",
    "    \n",
    "def remove_punctuation(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes all punctuation from it\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase without punctuation\n",
    "    \"\"\"\n",
    "    #https://www.tutorialspoint.com/python/string_maketrans.htm\n",
    "    #Check if NA\n",
    "    if type(phrase) is float:\n",
    "        if math.isnan(phrase):\n",
    "            return (\"\")\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    phrase = phrase.translate(translator) #removing punctuation\n",
    "        \n",
    "    return phrase\n",
    "\n",
    "def lemm_wordnet(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes lemmatizes it\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase in lemmas\n",
    "    \"\"\"\n",
    "    lemm = WordNetLemmatizer()\n",
    "    \n",
    "    #NA is a float type, so this if is to avoid conflict\n",
    "    if type(phrase) is not float:\n",
    "        phrase = [lemm.lemmatize(i) for i in phrase.split()]\n",
    "        phrase = ' '.join(phrase)\n",
    "    else:\n",
    "        return \"\"\n",
    "    return phrase\n",
    "    \n",
    "def remove_duplicate(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and removes all duplicate words\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase with just unique words\n",
    "    \"\"\"\n",
    "    aux_phrase = []\n",
    "        \n",
    "    if type(phrase) is not float:\n",
    "        \n",
    "        for i in phrase.split():\n",
    "            \n",
    "            if i not in aux_phrase:\n",
    "                aux_phrase.append(i)\n",
    "    \n",
    "    phrase = ' '.join(aux_phrase)\n",
    "    \n",
    "    return phrase\n",
    "    \n",
    "    \n",
    "def all_lower_case(phrase):    \n",
    "    \"\"\"\n",
    "    Receives a phrase and makes it lower case\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: The same phrase in lower case\n",
    "    \"\"\"\n",
    "    if type(phrase) is not float:\n",
    "            phrase = phrase.lower()\n",
    "    return phrase\n",
    "    \n",
    "def stem_snowball(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and returns the same phrase stemmed, lowercase phrase without stopwords\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: String. Stemmed, lowercase phrase without stopwords\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    #Stem words according to stemmer\n",
    "    final_phrase = []\n",
    "    words = phrase.split(\" \")\n",
    "    for word in words:\n",
    "        final_phrase.append((stemmer.stem(word)))\n",
    "    \n",
    "    final_phrase = ' '.join(final_phrase)\n",
    "    \n",
    "    return final_phrase\n",
    "\n",
    "stem_snowball(\"What is the step by step guide to invest in share market in india?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['len_q1'] = data_train.question1.apply(lambda x: len(str(x)))\n",
    "data_train['len_q2'] = data_train.question2.apply(lambda x: len(str(x)))\n",
    "data_train['diff_len'] = data_train.len_q1 - data_train.len_q2\n",
    "data_train['len_char_q1'] = data_train.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_train['len_char_q2'] = data_train.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_train['len_word_q1'] = data_train.question1.apply(lambda x: len(str(x).split()))\n",
    "data_train['len_word_q2'] = data_train.question2.apply(lambda x: len(str(x).split()))\n",
    "data_train['common_words'] = data_train.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection\\\n",
    "                                                            (set(str(x['question2']).lower().split()))), axis=1)\n",
    "data_train['prop_common_words'] = data_train.apply(lambda x: \\\n",
    "                                                   len(set(remove_punctuation(x['question1']).lower().split()).intersection\\\n",
    "                                                       (set(remove_punctuation(x['question2']).lower().split()))) / \\\n",
    "                                                   len(set(remove_punctuation(x['question1']).lower().split()).union\\\n",
    "                                                       (set(remove_punctuation(x['question2']).lower().split()))),axis=1 )\n",
    "\n",
    "\n",
    "data_train['fuzz_qratio'] = data_train.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_qratio')\n",
    "data_train['fuzz_WRatio'] = data_train.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_WRatio')\n",
    "data_train['fuzz_partial_ratio'] = data_train.apply(lambda x: fuzz.partial_ratio\\\n",
    "                                                    (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_partial_ratio')\n",
    "data_train['fuzz_partial_token_set_ratio'] = data_train.apply(lambda x: fuzz.partial_token_set_ratio\\\n",
    "                                                              (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_partial_token_set_ratio')\n",
    "data_train['fuzz_partial_token_sort_ratio'] = data_train.apply(lambda x: fuzz.partial_token_sort_ratio\\\n",
    "                                                               (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_partial_token_sort_ratio')\n",
    "data_train['fuzz_token_set_ratio'] = data_train.apply(lambda x: fuzz.token_set_ratio\\\n",
    "                                                      (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_token_set_ratio')\n",
    "data_train['fuzz_token_sort_ratio'] = data_train.apply(lambda x: fuzz.token_sort_ratio\\\n",
    "                                                       (str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "print('fuzz_token_sort_ratio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/seatgeek/fuzzywuzzy\n",
    "#https://pypi.python.org/pypi/fuzzywuzzy\n",
    "#http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.groupby('is_duplicate').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depois apagar as # para não ficar como comentário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data base of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving Pickle\n",
    "with open(os.path.join(save, 'train_features2.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save features as CSV\n",
    "data_train.to_csv(os.path.join(save, 'train_features_2.csv'),index=False)\n",
    "print(save)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data_train.info()\n",
    "\n",
    "#data_train_features = pd.read_csv(os.path.join(save, 'train_features.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "lines_count = 0\n",
    "a=set()\n",
    "for item in range(0,404290):\n",
    "    a =  a.union(\n",
    "     set(remove_punctuation(data_train.question1[item]).\\\n",
    "    split()).union(set(remove_punctuation(data_train.question2[item]).split())))\n",
    "    if item%50000==0:\n",
    "        print(item)\n",
    "print(len(a))\n",
    "'''\n",
    "#Resultado 136153 palavras única"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stemmers remove morphological affixes from words, leaving only the word stem.\n",
    "#http://www.nltk.org/howto/stem.html\n",
    "#The 'english' stemmer is better than the original 'porter' stemmer.\n",
    "#example; stemmer.stem('likely', 'bites') - like, bite \n",
    "#http://www.nltk.org/api/nltk.tokenize.html\n",
    "#A tokenizer that divides a string into substrings by splitting on the specified string (defined in subclasses).\n",
    "#word_tokenize(\"It's only a test\")- ['It', \"'s\", 'only', 'a', 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Editing questions with NLTK package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaning tool is used so you can easily choose which functions you want to use to clean te text\n",
    "def cleaning_tool(data, drop_na = True, lower_case = True, rm_duplicate = False, stopwords = False, \n",
    "                  punctuation = False, lemm = False, stem = False, list_of_stopwords = None):\n",
    "    \"\"\"\n",
    "    Function to process all data using calling functions from above, according to what was chosen.\n",
    "    :param data: data frame.\n",
    "    :param drop_na: If True drop all lines of data frame with NA\n",
    "    :param lower_case: If True transform for lower case\n",
    "    :param rm_duplicate: If True remove all duplicate words in questions\n",
    "    :param stopwords: If True removes stopwords\n",
    "    :param punctuation: If True removes punctuation\n",
    "    :param lemm: If True returns the phrase lemmatized\n",
    "    :param stem: If True returns the phrase stemmed\n",
    "    :param list_of_stopwords: List of stopwords to be used\n",
    "    :return: Question1 and Question2 processed according to parameters\n",
    "    \"\"\"\n",
    "    if drop_na == True:\n",
    "        data = data.dropna(0)\n",
    "    \n",
    "    if rm_duplicate == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_duplicate(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_duplicate(x))\n",
    "    \n",
    "    if lower_case == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: all_lower_case(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: all_lower_case(x))\n",
    "    \n",
    "    if stopwords == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_stopwords(x, list_of_stopwords))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_stopwords(x, list_of_stopwords))\n",
    "       \n",
    "    if punctuation == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_punctuation(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_punctuation(x))\n",
    "        \n",
    "    if lemm_wordnet == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: lemm_wordnet(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: lemm_wordnet(x))\n",
    "        \n",
    "    if stem_snowball == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: stem_snowball(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: stem_snowball(x))\n",
    "    \n",
    "    #We used it two times if some function create a new NA.\n",
    "    if drop_na == True:\n",
    "        data = data.dropna(0)    \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data_train_clean = cleaning_tool(data_train, stopwords=True, lemm=True,list_of_stopwords=mystopwords,punctuation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving Pickle\n",
    "with open(os.path.join(save, 'data_train_clean_features.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train_clean),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perguntar entre o split e word_tokenize\n",
    "#Verificar http://www.nltk.org/api/nltk.stem.html LEMATIZAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Plain Word Counts\n",
    "#X_traincv_tf, X_testcv_tf, y_traincv_tf, y_testcv_tf = model_selection.train_test_split(train_data_features_tf,\n",
    "                                                                                        train[\"sentiment\"],\n",
    "                                                                                        test_size=0.2,\n",
    "                                                                                        random_state=0)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1=data_train_clean[0:300000]\n",
    "sample2=data_train_clean.sample(300000)\n",
    "teste1=data_train_clean[300001:]\n",
    "teste2=data_train_clean.sample(104287)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#teste1.sample(2)\n",
    "#teste2.sample(2)\n",
    "#sample1.sample(2)\n",
    "#sample2.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(save, 'datas_sample.pkl'),'wb') as f:\n",
    "    pickle.dump((sample1,\n",
    "                 sample2,\n",
    "                 teste1,\n",
    "                 teste2),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer_tf = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 10000) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1_tf_question1 = vectorizer_tf.fit_transform(sample1.question1)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "sample1_tf_question1 = sample1_tf_question1.toarray()\n",
    "with open(os.path.join(save, 'sample1_tf_question1_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample1_tf_question1 ,f)\n",
    "del(sample1_tf_question1)\n",
    "sample2_tf_question1 = vectorizer_tf.fit_transform(sample2.question1)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "sample2_tf_question1 = sample2_tf_question1.toarray()\n",
    "with open(os.path.join(save, 'sample2_tf_question1_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample2_tf_question1 ,f)\n",
    "del(sample2_tf_question1)\n",
    "\n",
    "teste1_tf_question1 = vectorizer_tf.fit_transform(teste1.question1)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "teste1_tf_question1 = teste1_tf_question1.toarray()\n",
    "with open(os.path.join(save, 'teste1_tf_question1_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(teste1_tf_question1 ,f)\n",
    "del(teste1_tf_question1)\n",
    "    \n",
    "sample1_tf_question1 = vectorizer_tf.fit_transform(teste2.question1)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "teste2_tf_question1 = teste2_tf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste2_tf_question1_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(teste2_tf_question1 ,f)\n",
    "del(teste2_tf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1_tf_question2 = vectorizer_tf.fit_transform(sample1.question2)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "sample1_tf_question2 = sample1_tf_question2.toarray()\n",
    "with open(os.path.join(save, 'sample1_tf_question2_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample1_tf_question2 ,f)\n",
    "del(sample1_tf_question2)\n",
    "\n",
    "sample2_tf_question2 = vectorizer_tf.fit_transform(sample2.question2)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "sample2_tf_question2 = sample2_tf_question2.toarray()\n",
    "with open(os.path.join(save, 'sample2_tf_question2_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample2_tf_question2 ,f)\n",
    "del(sample2_tf_question2)\n",
    "\n",
    "teste1_tf_question2 = vectorizer_tf.fit_transform(teste1.question2)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "teste1_tf_question2 = teste1_tf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste1_tf_question1_vector2.pkl'),'wb') as f:\n",
    "    pickle.dump(teste1_tf_question2 ,f)\n",
    "del(teste1_tf_question2)\n",
    "\n",
    "teste2_tf_question2 = vectorizer_tf.fit_transform(teste2.question2)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "teste2_tf_question2 = teste2_tf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste2_tf_question2_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(teste2_tf_question2 ,f)\n",
    "del(teste2_tf_question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "#Another approach using TfIDf vectorizer and using the texts with stopwords in:\n",
    "#https://github.com/zygmuntz/classifying-text/blob/master/bow_predict.py \n",
    "vectorizer_tfidf = TfidfVectorizer(analyzer='word', \\\n",
    "                                  preprocessor=None,\\\n",
    "                                  tokenizer=None,\\\n",
    "                                  stop_words=None,\\\n",
    "                                  max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1_tfidf_question1 = vectorizer_tfidf.fit_transform(sample1.question1)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "sample1_tfidf_question1 = sample1_tfidf_question1.toarray()\n",
    "with open(os.path.join(save, 'sample1_tfidf_question1_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample1_tfidf_question1 ,f)\n",
    "del(sample1_tfidf_question1)\n",
    "\n",
    "sample2_tfidf_question1 = vectorizer_tfidf.fit_transform(sample2.question1)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "sample2_tfidf_question1 = sample2_tfidf_question1.toarray()\n",
    "with open(os.path.join(save, 'sample2_tfidf_question1_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample2_tfidf_question1 ,f)\n",
    "del(sample2_tfidf_question1)\n",
    "\n",
    "teste1_tfidf_question1 = vectorizer_tfidf.fit_transform(teste1.question1)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "teste1_tfidf_question1 = teste1_tfidf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste1_tfidf_question1_vector2.pkl'),'wb') as f:\n",
    "    pickle.dump(teste1_tfidf_question1,f)\n",
    "del(teste1_tfidf_question1)\n",
    "teste2_tfidf_question1 = vectorizer_tfidf.fit_transform(teste2.question1)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "teste2_tfidf_question1 = teste2_tfidf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste2_tfidf_question1_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(teste2_tfidf_question1 ,f)\n",
    "del(teste2_tfidf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1_tfidf_question2 = vectorizer_tfidf.fit_transform(sample1.question2)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "sample1_tfidf_question2 = sample1_tfidf_question2.toarray()\n",
    "with open(os.path.join(save, 'sample1_tfidf_question2_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample1_tfidf_question2 ,f)\n",
    "del(sample1_tfidf_question2)\n",
    "sample2_tfidf_question2 = vectorizer_tfidf.fit_transform(sample2.question2)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "sample2_tfidf_question2 = sample2_tfidf_question2.toarray()\n",
    "with open(os.path.join(save, 'sample2_tfidf_question2_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample2_tfidf_question2 ,f)\n",
    "del(sample2_tfidf_question2)\n",
    "teste1_tfidf_question2 = vectorizer_tfidf.fit_transform(teste1.question2)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "teste1_tfidf_question2 = teste1_tfidf_question2.toarray()\n",
    "with open(os.path.join(save, 'teste1_tfidf_question2_vector2.pkl'),'wb') as f:\n",
    "    pickle.dump(teste1_tfidf_question2,f)\n",
    "del(teste1_tfidf_question2)\n",
    "teste2_tfidf_question2 = vectorizer_tfidf.fit_transform(teste2.question2)\n",
    "teste2_tfidf_question2 = vectorizer_tfidf.fit_transform(teste2.question2)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "teste2_tfidf_question2 = teste2_tfidf_question2.toarray()\n",
    "with open(os.path.join(save, 'teste2_tfidf_question2_vector1.pkl'),'wb') as f:\n",
    "    pickle.dump(teste2_tfidf_question2 ,f)\n",
    "del(teste2_tfidf_question2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_clean_tf_question1 = vectorizer_tf.fit_transform(data_train_clean.question1)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "data_train_clean_tf_question1 = data_train_clean_tf_question1.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(save, 'data_train_clean_tf_question1_vector3.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train_clean_tf_question1,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_clean_tf_question2 = vectorizer_tf.fit_transform(data_train_clean.question2)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "data_train_clean_tf_question2 = data_train_clean_tf_question2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(save, 'data_train_clean_tf_question2_vector4.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train_clean_tf_question2,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_clean_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_clean_tfidf_question1 = vectorizer_tfidf.fit_transform(data_train_clean.question1)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "data_train_clean_tfidf_question1 = data_train_clean_tfidf.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(save, 'data_train_clean_tfidf_question1_vector5.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train_clean_tfidf_question1,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_clean_tfidf_question2 = vectorizer_tfidf.fit_transform(data_train_clean.question1)\n",
    "#data_train_clean_tf = data_train_clean_tf.toarray() # Numpy arrays are easy to work with\n",
    "#print(data_train_clean_tf.shape)\n",
    "data_train_clean_tfidf_question2 = data_train_clean_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(save, 'data_train_clean_tfidf_question2_vector6.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train_clean_tfidf_question2,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_clean_tfidf_question2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(save, 'data_train_features_vector10.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train_clean_tf_question1,\n",
    "                 data_train_clean_tf_question2,\n",
    "                 data_train_clean_tfidf_question1,\n",
    "                 data_train_clean_tfidf_question2),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''with open(os.path.join(outputs, 'data_train_features_vector2.pkl'),'rb') as f:\n",
    "    (train_data_features_tf, \n",
    "    test_data_features_tf,\n",
    "    train_data_features_tfidf,\n",
    "    test_data_features_tfidf) = pickle.load(f)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
