{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FGV EMAP\n",
    "\n",
    "## Modelagem e mineração de dados\n",
    "\n",
    "\n",
    "### Trabalho Kaggle Quora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alunos: Antonio Sombra e Joao Marcos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Carregando pacotes necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#strings\n",
    "import string\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "#Extras\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "#basic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import string\n",
    "import math\n",
    "\n",
    "\n",
    "# Visualisation\n",
    "import pylab\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregando banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>361422</th>\n",
       "      <td>What are some of your favourite poems?</td>\n",
       "      <td>What is your favourite poem and why?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187384</th>\n",
       "      <td>Why sex is so important in life?</td>\n",
       "      <td>Why is sex given such importance in relationsh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130008</th>\n",
       "      <td>Do employees at Inventure Foods have a good wo...</td>\n",
       "      <td>Do employees at B&amp;G Foods have a good work-lif...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question1  \\\n",
       "361422             What are some of your favourite poems?   \n",
       "187384                   Why sex is so important in life?   \n",
       "130008  Do employees at Inventure Foods have a good wo...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "361422               What is your favourite poem and why?             1  \n",
       "187384  Why is sex given such importance in relationsh...             0  \n",
       "130008  Do employees at B&G Foods have a good work-lif...             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = \"/Dados/Kaggle/\" #Diretório da base de dados\n",
    "word2vec= \"/Dados/Word2vec\" #diretório word2vec\n",
    "save=  \"/home/joaomarcosest/Kaggle_Quora/Dados\" #diterório para criação de dados auxiliares\n",
    "data_train = pd.read_csv(os.path.join(datapath, 'train.csv'))\n",
    "data_train=data_train.drop(['id','qid1','qid2'],axis=1)# #Eliminando colunas desnecessária à análise\n",
    "data_train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404290 entries, 0 to 404289\n",
      "Data columns (total 3 columns):\n",
      "question1       404290 non-null object\n",
      "question2       404288 non-null object\n",
      "is_duplicate    404290 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 9.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404290.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.369198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.482588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_duplicate\n",
       "count  404290.000000\n",
       "mean        0.369198\n",
       "std         0.482588\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         1.000000\n",
       "max         1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.info()\n",
    "data_train.describe()\n",
    "\n",
    "#0.369198% do banco são de questõs clássificadas como duplicadas\n",
    "# O banco possui duas questõe NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(data_train.columns)# Verificando as colunas do banco\n",
    "type(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choosing the list of stopwords\n",
    "mystopwords = nltk.corpus.stopwords.words('english')#carregando stop words\n",
    "list_of_words= ['where','what','when','why','between','who','how','which']#Eliminando palavras da lista de stopwords\n",
    "for item in list_of_words:\n",
    "        mystopwords.remove(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funçoes necessárias para limpeza do banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the step by step guid to invest in share market in india?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(phrase,list_stopwords):\n",
    "    \"\"\"\n",
    "    Função recebe uma frase e uma lista de stopwords\n",
    "    :return: Retorna a frase sem stopwords\n",
    "    \"\"\"\n",
    "    final_phrase = []\n",
    "    words = phrase.split(\" \")\n",
    "    for word in words:\n",
    "        if word not in list_stopwords:\n",
    "            final_phrase.append((word))\n",
    "    \n",
    "    final_phrase = ' '.join(final_phrase)\n",
    "    \n",
    "    return final_phrase\n",
    "    \n",
    "def remove_punctuation(phrase):\n",
    "    \"\"\"\n",
    "    Função recebe uma frase e retorna a mesma sem pontuações.\n",
    "    :return: Retorna a prase sem pontuações.\n",
    "    \"\"\"\n",
    "    #https://www.tutorialspoint.com/python/string_maketrans.htm\n",
    "    #Check if NA\n",
    "    if type(phrase) is float:\n",
    "        if math.isnan(phrase):\n",
    "            return (\"\")\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    phrase = phrase.translate(translator) #removing punctuation\n",
    "        \n",
    "    return phrase\n",
    "\n",
    "def lemm_wordnet(phrase):\n",
    "    \"\"\"\n",
    "    Lematiza as palavras da frase\n",
    "    \"\"\"\n",
    "    lemm = WordNetLemmatizer()\n",
    "    \n",
    "    #NA is a float type, so this if is to avoid conflict\n",
    "    if type(phrase) is not float:\n",
    "        phrase = [lemm.lemmatize(i) for i in phrase.split()]\n",
    "        phrase = ' '.join(phrase)\n",
    "    else:\n",
    "        return \"\"\n",
    "    return phrase\n",
    "    \n",
    "def remove_duplicate(phrase):\n",
    "    \"\"\"\n",
    "    remove palavras duplicadas dentro de uma frase\n",
    "    \"\"\"\n",
    "    aux_phrase = []\n",
    "        \n",
    "    if type(phrase) is not float:\n",
    "        \n",
    "        for i in phrase.split():\n",
    "            \n",
    "            if i not in aux_phrase:\n",
    "                aux_phrase.append(i)\n",
    "    \n",
    "    phrase = ' '.join(aux_phrase)\n",
    "    \n",
    "    return phrase\n",
    "    \n",
    "    \n",
    "def all_lower_case(phrase):    \n",
    "    \"\"\"\n",
    "   Transforma toda frase para lowwer_case, i.e deixa a frase toda em minusculo\n",
    "    \"\"\"\n",
    "    if type(phrase) is not float:\n",
    "            phrase = phrase.lower()\n",
    "    return phrase\n",
    "    \n",
    "def stem_snowball(phrase):\n",
    "    \"\"\"\n",
    "    Receives a phrase and returns the same phrase stemmed, lowercase phrase without stopwords\n",
    "    :param phrase: String. A phrase.\n",
    "    :return: String. Stemmed, lowercase phrase without stopwords\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    #Stem words according to stemmer\n",
    "    final_phrase = []\n",
    "    words = phrase.split(\" \")\n",
    "    for word in words:\n",
    "        final_phrase.append((stemmer.stem(word)))\n",
    "    \n",
    "    final_phrase = ' '.join(final_phrase)\n",
    "    \n",
    "    return final_phrase\n",
    "\n",
    "stem_snowball(\"What is the step by step guide to invest in share market in india?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuzz_qratio\n",
      "fuzz_WRatio\n",
      "fuzz_partial_ratio\n",
      "fuzz_partial_token_set_ratio\n",
      "fuzz_partial_token_sort_ratio\n",
      "fuzz_token_set_ratio\n",
      "fuzz_token_sort_ratio\n"
     ]
    }
   ],
   "source": [
    "#Conta  o numero de caracteres em cada frases, considerando tbm os espacos em branco\n",
    "data_train['len_q1'] = data_train.question1.apply(lambda x: len(str(x)))\n",
    "data_train['len_q2'] = data_train.question2.apply(lambda x: len(str(x)))\n",
    "#diferenca de caracteres da primeira questao com a segunda\n",
    "data_train['diff_len'] = data_train.len_q1 - data_train.len_q2\n",
    "#contao numero de caracteres unico em cada frase, ignorando os espacos em branco\n",
    "data_train['len_char_q1'] = data_train.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data_train['len_char_q2'] = data_train.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "#Conta o numero de palavras em cada questao\n",
    "data_train['len_word_q1'] = data_train.question1.apply(lambda x: len(str(x).split()))\n",
    "data_train['len_word_q2'] = data_train.question2.apply(lambda x: len(str(x).split()))\n",
    "#Numero de palavras em comum nas duas frases\n",
    "data_train['common_words'] = data_train.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection\\\n",
    "                                                            (set(str(x['question2']).lower().split()))), axis=1)\n",
    "#Proporcao de palavras em comum nas duas frases\n",
    "data_train['prop_common_words'] = data_train.apply(lambda x: \\\n",
    "                                                   len(set(remove_punctuation(x['question1']).lower().split()).intersection\\\n",
    "                                                       (set(remove_punctuation(x['question2']).lower().split()))) / \\\n",
    "                                                   len(set(remove_punctuation(x['question1']).lower().split()).union\\\n",
    "                                                       (set(remove_punctuation(x['question2']).lower().split()))),axis=1 )\n",
    "\n",
    "#https://github.com/seatgeek/fuzzywuzzy\n",
    "# calcula distancias de Levensthein para as duas questoes\n",
    "data_train['fuzz_qratio'] = data_train.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_qratio')\n",
    "\n",
    "data_train['fuzz_WRatio'] = data_train.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_WRatio')\n",
    "\n",
    "data_train['fuzz_partial_ratio'] = data_train.apply(lambda x: fuzz.partial_ratio\\\n",
    "                                                    (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_partial_ratio')\n",
    "\n",
    "data_train['fuzz_partial_token_set_ratio'] = data_train.apply(lambda x: fuzz.partial_token_set_ratio\\\n",
    "                                                              (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_partial_token_set_ratio')\n",
    "\n",
    "data_train['fuzz_partial_token_sort_ratio'] = data_train.apply(lambda x: fuzz.partial_token_sort_ratio\\\n",
    "                                                               (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_partial_token_sort_ratio')\n",
    "\n",
    "data_train['fuzz_token_set_ratio'] = data_train.apply(lambda x: fuzz.token_set_ratio\\\n",
    "                                                      (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_token_set_ratio')\n",
    "\n",
    "data_train['fuzz_token_sort_ratio'] = data_train.apply(lambda x: fuzz.token_sort_ratio\\\n",
    "                                                       (str(x['question1']), str(x['question2'])), axis=1)\n",
    "print('fuzz_token_sort_ratio')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicacao sobre as funcoes fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/seatgeek/fuzzywuzzy\n",
    "#https://pypi.python.org/pypi/fuzzywuzzy\n",
    "#http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">common_words</th>\n",
       "      <th colspan=\"2\" halign=\"left\">diff_len</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">len_word_q2</th>\n",
       "      <th colspan=\"8\" halign=\"left\">prop_common_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_duplicate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255027.0</td>\n",
       "      <td>3.960922</td>\n",
       "      <td>3.199845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>255027.0</td>\n",
       "      <td>-0.849134</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>255027.0</td>\n",
       "      <td>0.300786</td>\n",
       "      <td>0.239106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149263.0</td>\n",
       "      <td>5.452436</td>\n",
       "      <td>2.666256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>149263.0</td>\n",
       "      <td>-0.097586</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>149263.0</td>\n",
       "      <td>0.466475</td>\n",
       "      <td>0.206193</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             common_words                                                \\\n",
       "                    count      mean       std  min  25%  50%  75%   max   \n",
       "is_duplicate                                                              \n",
       "0                255027.0  3.960922  3.199845  0.0  2.0  3.0  5.0  34.0   \n",
       "1                149263.0  5.452436  2.666256  1.0  4.0  5.0  7.0  41.0   \n",
       "\n",
       "              diff_len           ...  len_word_q2        prop_common_words  \\\n",
       "                 count      mean ...          75%    max             count   \n",
       "is_duplicate                     ...                                         \n",
       "0             255027.0 -0.849134 ...         14.0  237.0          255027.0   \n",
       "1             149263.0 -0.097586 ...         11.0   60.0          149263.0   \n",
       "\n",
       "                                                                               \n",
       "                  mean       std       min       25%       50%       75%  max  \n",
       "is_duplicate                                                                   \n",
       "0             0.300786  0.239106  0.000000  0.121212  0.230769  0.428571  1.0  \n",
       "1             0.466475  0.206193  0.055556  0.307692  0.437500  0.600000  1.0  \n",
       "\n",
       "[2 rows x 128 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.groupby('is_duplicate').describe()#Verificando as estatisticas por grupo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando a base com as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404290 entries, 0 to 404289\n",
      "Data columns (total 19 columns):\n",
      "question1                        404290 non-null object\n",
      "question2                        404288 non-null object\n",
      "is_duplicate                     404290 non-null int64\n",
      "len_q1                           404290 non-null int64\n",
      "len_q2                           404290 non-null int64\n",
      "diff_len                         404290 non-null int64\n",
      "len_char_q1                      404290 non-null int64\n",
      "len_char_q2                      404290 non-null int64\n",
      "len_word_q1                      404290 non-null int64\n",
      "len_word_q2                      404290 non-null int64\n",
      "common_words                     404290 non-null int64\n",
      "prop_common_words                404290 non-null float64\n",
      "fuzz_qratio                      404290 non-null int64\n",
      "fuzz_WRatio                      404290 non-null int64\n",
      "fuzz_partial_ratio               404290 non-null int64\n",
      "fuzz_partial_token_set_ratio     404290 non-null int64\n",
      "fuzz_partial_token_sort_ratio    404290 non-null int64\n",
      "fuzz_token_set_ratio             404290 non-null int64\n",
      "fuzz_token_sort_ratio            404290 non-null int64\n",
      "dtypes: float64(1), int64(16), object(2)\n",
      "memory usage: 58.6+ MB\n"
     ]
    }
   ],
   "source": [
    "#Verificando os dados apos a insercao das features\n",
    "data_train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Salvando pickle da base de dados com as features para nao precisar rodar novamente as features\n",
    "with open(os.path.join(save, 'train_features.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Salvando o banco de treino com as features em csv\n",
    "data_train.to_csv(os.path.join(save, 'train_features.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Conta o numero total de palavras unica no banco de dados de treinamento\n",
    "'''\n",
    "lines_count = 0\n",
    "a=set()\n",
    "for item in range(0,404290):\n",
    "    a =  a.union(\n",
    "     set(remove_punctuation(data_train.question1[item]).\\\n",
    "    split()).union(set(remove_punctuation(data_train.question2[item]).split())))\n",
    "    if item%50000==0:\n",
    "        print(item)\n",
    "print(len(a))\n",
    "'''\n",
    "#Resultado 136153 palavras única"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stemmers remove morphological affixes from words, leaving only the word stem.\n",
    "#http://www.nltk.org/howto/stem.html\n",
    "#The 'english' stemmer is better than the original 'porter' stemmer.\n",
    "#example; stemmer.stem('likely', 'bites') - like, bite \n",
    "#http://www.nltk.org/api/nltk.tokenize.html\n",
    "#A tokenizer that divides a string into substrings by splitting on the specified string (defined in subclasses).\n",
    "#word_tokenize(\"It's only a test\")- ['It', \"'s\", 'only', 'a', 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Limpando a base de dados para vetorizar e criar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Funcao que voce escolhe quais filtros deseja fazer no seu banco de dados, lematizar, stematizar, remover duplicadas\n",
    "#remover pontuacoes, remover stopwords, remover na e pasar todas palavras para minusculo\n",
    "def cleaning_tool(data, drop_na = True, lower_case = True, rm_duplicate = False, stopwords = False, \n",
    "                  punctuation = False, lemma = False, stem = False, list_of_stopwords = None):\n",
    "    \"\"\"\n",
    "    Function to process all data using calling functions from above, according to what was chosen.\n",
    "    :param data: data frame.\n",
    "    :param drop_na: If True drop all lines of data frame with NA\n",
    "    :param lower_case: If True transform for lower case\n",
    "    :param rm_duplicate: If True remove all duplicate words in questions\n",
    "    :param stopwords: If True removes stopwords\n",
    "    :param punctuation: If True removes punctuation\n",
    "    :param lemm: If True returns the phrase lemmatized\n",
    "    :param stem: If True returns the phrase stemmed\n",
    "    :param list_of_stopwords: List of stopwords to be used\n",
    "    :return: Question1 and Question2 processed according to parameters\n",
    "    \"\"\"\n",
    "    if drop_na == True:\n",
    "        data = data.dropna(0)\n",
    "    \n",
    "    if rm_duplicate == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_duplicate(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_duplicate(x))\n",
    "    \n",
    "    if lower_case == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: all_lower_case(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: all_lower_case(x))\n",
    "    \n",
    "    if stopwords == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_stopwords(x, list_of_stopwords))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_stopwords(x, list_of_stopwords))\n",
    "       \n",
    "    if punctuation == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: remove_punctuation(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: remove_punctuation(x))\n",
    "        \n",
    "    if lemma == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: lemm_wordnet(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: lemm_wordnet(x))\n",
    "        \n",
    "    if stem_snowball == True:\n",
    "        data[\"question1\"] = data[\"question1\"].apply(lambda x: stem_snowball(x))\n",
    "        data[\"question2\"] = data[\"question2\"].apply(lambda x: stem_snowball(x))\n",
    "    \n",
    "    #We used it two times if some function create a new NA.\n",
    "    if drop_na == True:\n",
    "        data = data.dropna(0)    \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#um banco de dados limpo usando apenas lematizacao e um usando ematizacao e stematizacao\n",
    "data_train_clean = cleaning_tool(data_train, stopwords=True, lemma=True,list_of_stopwords=mystopwords,punctuation=True)\n",
    "data_train_clean2 = cleaning_tool(data_train, stopwords=True, lemma=True,list_of_stopwords=mystopwords,punctuation=True,stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Salvando pickle dos dois bancos limpos\n",
    "with open(os.path.join(save, 'data_train_clean_features.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train_clean),f)\n",
    "with open(os.path.join(save, 'data_train_clean_features2.pkl'),'wb') as f:\n",
    "    pickle.dump((data_train_clean2),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#salvando como csv\n",
    "data_train_clean.to_csv(os.path.join(save, 'train_clean_features.csv'),index=False)\n",
    "data_train_clean2.to_csv(os.path.join(save, 'train_clean_features2.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Amostras do banco para validar o modelo\n",
    "sample1=data_train_clean[0:300000]\n",
    "sample2=data_train_clean.sample(300000)\n",
    "teste1=data_train_clean[300001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#salvando amostras para valiadar o modelo\n",
    "with open(os.path.join(save, 'datas_sample.pkl'),'wb') as f:\n",
    "    pickle.dump((sample1,\n",
    "                 sample2,\n",
    "                 teste1),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetorizando a base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Como o banco tem mais de 130 mil palavras escolhemos trabalhar com 10000\n",
    "\n",
    "vectorizer_tf = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             ngram_range =(1,3), \\\n",
    "                             max_features = 10000) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_TF data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = data_train_clean.question1.append([data_train_clean.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tf.fit(questions)\n",
    "data_train_clean_tf_question1 = vector_fitt.transform(data_train_clean.question1)\n",
    "# Numpy arrays are easy to work with\n",
    "#data_train_clean_tf_question1 = data_train_clean_tf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save,'data_train_clean_tf_question1.pkl'),'wb') as f:\n",
    "    pickle.dump(data_train_clean_tf_question1,f)\n",
    "del(data_train_clean_tf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_clean_tf_question2 = vector_fitt.transform(data_train_clean.question2)\n",
    "# Numpy arrays are easy to work with\n",
    "#data_train_clean_tf_question2 = data_train_clean_tf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'data_train_clean_tf_question2.pkl'),'wb') as f:\n",
    "    pickle.dump(data_train_clean_tf_question2,f)\n",
    "del(data_train_clean_tf_question2)\n",
    "del(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_TF sample1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1_questions=sample1.question1.append([sample1.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tf.fit(sample1_questions)\n",
    "sample1_tf_question1 = vector_fitt.transform(sample1.question1)\n",
    "# Numpy arrays are easy to work with\n",
    "#sample1_tf_question1 = sample1_tf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample1_tf_question1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample1_tf_question1,f)\n",
    "del(sample1_tf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1_tf_question2 = vector_fitt.transform(sample1.question2)\n",
    "#sample1_tf_question2 = sample1_tf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample1_tf_question2.pkl'),'wb') as f:\n",
    "    pickle.dump(sample1_tf_question2,f)\n",
    "del(sample1_tf_question2)\n",
    "del(sample1_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_TF sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample2_questions=sample2.question1.append([sample1.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tf.fit(sample2_questions)\n",
    "sample2_tf_question1 = vector_fitt.transform(sample2.question1)\n",
    "#sample2_tf_question1 = sample2_tf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample2_tf_question1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample2_tf_question1,f)\n",
    "del(sample2_tf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample2_tf_question2 = vector_fitt.transform(sample2.question2)\n",
    "#sample2_tf_question2 = sample2_tf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample2_tf_question2.pkl'),'wb') as f:\n",
    "    pickle.dump(sample2_tf_question2,f)\n",
    "del(sample2_tf_question2)\n",
    "del(sample2_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_TF teste1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teste1_questions=teste1.question1.append([teste1.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tf.fit(teste1_questions)\n",
    "teste1_tf_question1 = vector_fitt.transform(teste1.question1)\n",
    "#teste1_tf_question1 = teste1_tf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste1_tf_question1.pkl'),'wb') as f:\n",
    "    pickle.dump(teste1_tf_question1,f)\n",
    "del(teste1_tf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tf.fit(teste1_questions)\n",
    "teste1_tf_question2 = vector_fitt.transform(teste1.question2)\n",
    "#teste1_tf_question2 = teste1_tf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste1_tf_question2.pkl'),'wb') as f:\n",
    "    pickle.dump(teste1_tf_question2,f)\n",
    "del(teste1_tf_question2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://scikhttps://jupyterhub.namd.mat.br/user/joaomarcosest/treeit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "#Another approach using TfIDf vectorizer and using the texts with stopwords in:\n",
    "#https://github.com/zygmuntz/classifying-text/blob/master/bow_predict.py \n",
    "vectorizer_tfidf = TfidfVectorizer(analyzer='word', \\\n",
    "                                  preprocessor=None,\\\n",
    "                                  tokenizer=None,\\\n",
    "                                  stop_words=None,\\\n",
    "                                 ngram_range =(1,3), \\\n",
    "                                  max_features=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_TFIDF data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = data_train_clean.question1.append([data_train_clean.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tfidf.fit(questions)\n",
    "data_train_clean_tfidf_question1 = vector_fitt.transform(data_train_clean.question1)\n",
    "# Numpy arrays are easy to work with\n",
    "#data_train_clean_tfidf_question1 = data_train_clean_tfidf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save,'data_train_clean_tfidf_question1.pkl'),'wb') as f:\n",
    "    pickle.dump(data_train_clean_tfidf_question1,f)\n",
    "del(data_train_clean_tfidf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_clean_tfidf_question2 = vector_fitt.transform(data_train_clean.question2)\n",
    " # Numpy arrays are easy to work with\n",
    "#data_train_clean_tfidf_question2 = data_train_clean_tfidf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'data_train_clean_tfidf_question2.pkl'),'wb') as f:\n",
    "    pickle.dump(data_train_clean_tfidf_question2,f)\n",
    "del(data_train_clean_tfidf_question2)\n",
    "del(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_tfidf sample1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1_questions=sample1.question1.append([sample1.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tfidf.fit(sample1_questions)\n",
    "sample1_tfidf_question1 = vector_fitt.transform(sample1.question1)\n",
    "# Numpy arrays are easy to work with\n",
    "#sample1_tfidf_question1 = sample1_tfidf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample1_tfidf_question1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample1_tfidf_question1,f)\n",
    "del(sample1_tfidf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1_tfidf_question2 = vector_fitt.transform(sample1.question2)\n",
    "#sample1_tfidf_question2 = sample1_tfidf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample1_tfidf_question2.pkl'),'wb') as f:\n",
    "    pickle.dump(sample1_tfidf_question2,f)\n",
    "del(sample1_tfidf_question2)\n",
    "del(sample1_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_tfidf sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample2_questions=sample2.question1.append([sample1.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tfidf.fit(sample2_questions)\n",
    "sample2_tfidf_question1 = vector_fitt.transform(sample2.question1)\n",
    "#sample2_tfidf_question1 = sample2_tfidf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample2_tfidf_question1.pkl'),'wb') as f:\n",
    "    pickle.dump(sample2_tfidf_question1,f)\n",
    "del(sample2_tfidf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample2_tfidf_question2 = vector_fitt.transform(sample2.question2)\n",
    "#sample2_tfidf_question2 = sample2_tfidf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'sample2_tfidf_question2.pkl'),'wb') as f:\n",
    "    pickle.dump(sample2_tfidf_question2,f)\n",
    "del(sample2_tfidf_question2)\n",
    "del(sample2_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize_tfidf teste1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teste1_questions=teste1.question1.append([teste1.question2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tfidf.fit(teste1_questions)\n",
    "teste1_tfidf_question1 = vector_fitt.transform(teste1.question1)\n",
    "#teste1_tfidf_question1 = teste1_tfidf_question1.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste1_tfidf_question1.pkl'),'wb') as f:\n",
    "    pickle.dump(teste1_tfidf_question1,f)\n",
    "del(teste1_tfidf_question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_fitt = vectorizer_tfidf.fit(teste1_questions)\n",
    "teste1_tfidf_question2 = vector_fitt.transform(teste1.question2)\n",
    "#teste1_tfidf_question2 = teste1_tfidf_question2.toarray()\n",
    "\n",
    "with open(os.path.join(save, 'teste1_tfidf_question2.pkl'),'wb') as f:\n",
    "    pickle.dump(teste1_tfidf_question2,f)\n",
    "del(teste1_tfidf_question2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Abrindo os vetores das das frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(save, 'train_features.pkl'),'rb') as f:\n",
    "    (train_data_features_tf) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NAO CONSEGUIMOS PASSAR PARA ARRAY OS VETORES DE PALAVRAS POIS DAVA ERRO DE MEMORIA!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
